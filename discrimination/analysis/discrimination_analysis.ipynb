{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matched Discrimination Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "import scipy.stats as stats\n",
    "from scipy.stats import sem \n",
    "from mpl_toolkits.mplot3d import Axes3D # <--- This is important for 3d plotting \n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineCSVs(datafolder, num_discrim_trials):\n",
    "    \"\"\"\n",
    "    Combine all participant data into one pandas df\n",
    "    OR \n",
    "    Create df for single participant file \n",
    "    \n",
    "    returns:\n",
    "        (1) combined dataframe of all discrimination data \n",
    "        (2) combined dataframe of all scene property rating data \n",
    "    \"\"\"\n",
    "    #checks if path is a file\n",
    "    isFile = os.path.isfile(datafolder)\n",
    "\n",
    "    #checks if path is a directory\n",
    "    \n",
    "    isDirectory = os.path.isdir(datafolder)\n",
    "    \n",
    "    if isDirectory == True:\n",
    "        discrim_data = []\n",
    "        for filename in os.listdir(datafolder):\n",
    "            if 'csv' in filename:\n",
    "                path = datafolder + \"/\" + filename\n",
    "                df = pd.read_csv(path, index_col=None, header=0)\n",
    "                \n",
    "                df_discrim = df[0:num_discrim_trials]\n",
    "                discrim_data.append(df_discrim)\n",
    "\n",
    "        discrim_frame = pd.concat(discrim_data, axis=0, ignore_index=True)\n",
    "        \n",
    "    if isFile == True:\n",
    "        if 'csv' in datafolder:\n",
    "            df = pd.read_csv(datafolder, index_col=None, header=0)\n",
    "            df_discrim = df[0:num_discrim_trials]\n",
    "            discrim_data.append(df_discrim)\n",
    " \n",
    "    return discrim_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''\n",
    "num_total_trials = 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_discrim, raw_ratings = combineCSVs(data_path, num_total_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_discrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjIDs = raw_discrim.subjID.unique()\n",
    "len(all_subjIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ages = raw_discrim.age.unique()\n",
    "all_ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Data Cleaning </font> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catchTrial_cleaning(df, correct_requirement, catch_stimuli, sequences_count_dict):\n",
    "    '''\n",
    "    Participants complete 8 catch trials total to ensure that they are doing the task.\n",
    "    If less than 6/8 catch trials are correct, the participant is excluded.  \n",
    "    '''\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    remove = []\n",
    "    subj_sequence = {}\n",
    "    df2_list = []\n",
    "    \n",
    "    for subj in all_subjIDs:\n",
    "        count_correct = 0\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning\n",
    "        # \n",
    "        subj_sequence[subj] = subj_df.sequenceName.unique()[0]\n",
    "        \n",
    "        # remove trials that are outside the outlier range\n",
    "        c = 0\n",
    "        c_missed = 0\n",
    "        for idx, row in subj_df.iterrows():\n",
    "            stim1 = row['stimulus_0']\n",
    "            stim2 = row['stimulus_1']\n",
    "            # TEMP SOLUTION FOR RANDOM PHP DATA SAVING PROBLEM \n",
    "            if type(stim1) == str:\n",
    "                if stim1.split('/')[1] in catch_stimuli or stim2.split('/')[1] in catch_stimuli:\n",
    "                    ####### VERSION WHERE CATCH TRIALS ARE ATTENTION CHECK: IMAGE 1 IS THE SAME AS IMAGE 2\n",
    "                    c += 1\n",
    "                    if row[\"discrim_choice\"] == 3:\n",
    "#                         print(row[\"discrim_choice\"])\n",
    "                        count_correct += 1\n",
    "#                     else:\n",
    "#                         print(row[\"discrim_choice\"])\n",
    "                    # remove catch trial \n",
    "                    cleaned_subj_df.drop([idx], inplace=True)\n",
    "    #                 print(depth0, depth1, correct_choice, choice)\n",
    "#         print(c_missed, 'Number of catch trials where participants did not see the target')\n",
    "#         print(c)\n",
    "#         print(count_correct)\n",
    "        if count_correct < correct_requirement:\n",
    "#             print('Number correct:', count_correct)\n",
    "            remove.append(subj)\n",
    "        else:\n",
    "            sequence_count_dict[subj_df.sequenceName.unique()[0]] += 1\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "    print(\"Number of participants that did not pass the catch trial check:\", len(remove))\n",
    "    print(\"Participants that were removed:\",remove)\n",
    "#     print(\"Sequences that need to be replaced:\")\n",
    "#     for subj in remove:\n",
    "#         print(subj_sequence[subj])\n",
    "#     print(\"Sequence count:\")\n",
    "#     print(sequence_count)\n",
    "    for index, row in df2.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df2.drop(index, inplace=True)\n",
    "    \n",
    "    return df2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/v2_depth_discrimination_MTurk/jsons'\n",
    "sequences_count_dict = {}\n",
    "for seq in os.listdir(sequences_path):\n",
    "    if 'json' in seq:\n",
    "        sequences_count_dict['jsons/'+seq] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_catch_stim = ['000375_2014-06-08_11-17-29_260595134347_rgbf000133-resize_2',\n",
    "                  '000569_2014-06-09_22-51-47_260595134347_rgbf000141-resize_3',\n",
    "                  '000787_2014-06-08_22-33-53_260595134347_rgbf000175-resize_1',\n",
    "                  '002072_2014-06-24_21-48-06_260595134347_rgbf000115-resize_0',\n",
    "                  '001170_2014-06-17_15-43-44_260595134347_rgbf000096-resize_6',\n",
    "                  '001222_2014-06-17_16-24-06_260595134347_rgbf000073-resize_0',\n",
    "                  '001498_2014-06-19_17-45-14_260595134347_rgbf000129-resize_4',\n",
    "                  '001540_2014-06-20_17-01-05_260595134347_rgbf000086-resize_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_cleaned_discrim = catchTrial_cleaning(raw_discrim, 6, all_catch_stim, sequences_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ages = catch_cleaned_discrim.age.unique()\n",
    "cleaned_ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RT_cleaning(df, outlier_range, num_trials):\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    remove = []\n",
    "    df2_list = []\n",
    "    for subj in all_subjIDs:\n",
    "        count = 0\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning\n",
    "        \n",
    "        # calculate subject's average trial RT\n",
    "        average_trial_RT = subj_df[\"trial_RT\"].mean()\n",
    "        std_trial_RT = subj_df[\"trial_RT\"].std()\n",
    "        \n",
    "        # remove trials that are outside the outlier range\n",
    "        for idx, row in subj_df.iterrows():\n",
    "            RT = row['trial_RT']\n",
    "            if RT < outlier_range[0]:\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "            if RT > outlier_range[1]:\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "                \n",
    "        threshold = math.floor(num_trials * 0.1)\n",
    "        if count >= threshold:\n",
    "            remove.append(subj)\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "    print(\"Number of Participants with 10% or more trials outside their RT range:\", len(remove))\n",
    "    \n",
    "    for index, row in df2.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df2.drop(index, inplace=True)\n",
    "    \n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_discrim = RT_cleaning(catch_cleaned_discrim, [250,10000], 78)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalTrialCountCheck(df, num_trials):\n",
    "    \"\"\"\n",
    "    If more then 10% of a participants data is missing, remove the participant\n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\\n\",\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    remove = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        count_trials = len(subj_df.index)\n",
    "        threshold_trials_remaining = num_trials - math.floor(num_trials * 0.1)\n",
    "        \n",
    "        if count_trials <= threshold_trials_remaining:\n",
    "            remove.append(subj)\n",
    "    print(\"Number of Participants with >= 10% trials removed:\", len(remove))\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df.drop(index, inplace=True)\n",
    "    \n",
    "    print(\"Number of participants left:\",len(df.subjID.unique()))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discrim = finalTrialCountCheck(cleaned_discrim, 78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discrim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score RT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscored_outcomes(df):\n",
    "    '''\n",
    "    z-score RTs:\n",
    "        for each subj calculate their avg and std \n",
    "        zscored = (estim - subj avg)/subj std\n",
    "    '''\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    df2_list = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        final_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning \n",
    "\n",
    "        # Z-Score RT\n",
    "        stimulus_duration = subj_df.log_sceneDuration2.unique()[0]\n",
    "        average_RT = subj_df[\"trial_RT\"].mean()\n",
    "        std_RT = subj_df[\"trial_RT\"].std()\n",
    "        subj_RTs = np.array(list(subj_df[\"trial_RT\"])) - stimulus_duration\n",
    "        zscored_subj_RTs = (subj_RTs - average_RT)/std_RT\n",
    "        final_subj_df.replace(subj_RTs, zscored_subj_RTs, inplace=True)\n",
    "        df2_list.append(final_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)    \n",
    "\n",
    "    return df2\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscored_data = zscored_outcomes(final_discrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscored_RT = zscored_data['trial_RT']\n",
    "final_discrim_RT = final_discrim['trial_RT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences that are completed\n",
    "\n",
    "sampled_urls = zscored_data.url.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Sequences that need replacing\n",
    "\n",
    "Final N = 192 \n",
    "\n",
    "Each sequence should have 48 participants -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final list of subjects \n",
    "zscored_data.subjID.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation with Verbal Report data \n",
    "\n",
    "- For each discrimination trial, get the average difference between the verbal report for those scenes \n",
    "- Correlate that with the proportion correct for that trial \n",
    "- Is there better accuracy for scenes that have a larger percevied depth difference (using the verbal report data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Discriminations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(df):\n",
    "    \"\"\"\n",
    "    * ACCURACY BASED ON THE KINECT\n",
    "    args:\n",
    "        df  \n",
    "    returns:\n",
    "        proportion of correct responses, count of correct responses, count of total trials  \n",
    "    \"\"\"\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    count_total = 0\n",
    "    count_missed = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        choice = row[\"discrim_choice\"]\n",
    "        if choice == 2.0:\n",
    "            count_missed += 1\n",
    "        else:    \n",
    "            count_total += 1\n",
    "            depth0 = row[\"actual_depth_0\"]\n",
    "            depth1 = row[\"actual_depth_1\"]\n",
    "            if depth0 < depth1:\n",
    "                correct_choice = 0\n",
    "            if depth0 > depth1:\n",
    "                correct_choice = 1\n",
    "            if depth0 == depth1:\n",
    "                # case where depths are equal \n",
    "                correct_choice = None\n",
    "            if choice == correct_choice:\n",
    "                count_correct += 1\n",
    "    \n",
    "    return count_correct/count_total, count_correct, count_total, count_missed\n",
    "\n",
    "def get_RT(df):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        df  \n",
    "    returns:\n",
    "        array of RTs, avg RT and std   \n",
    "    \"\"\"\n",
    "    list_RTs = []\n",
    "    for idx, row in df.iterrows():   \n",
    "        list_RTs.append(row[\"trial_RT\"])\n",
    "    \n",
    "    list_RTs = np.array(list_RTs)\n",
    "    \n",
    "    return list_RTs, np.mean(list_RTs) ,np.std(list_RTs), stats.sem(list_RTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_differences(df):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        df  \n",
    "    returns:\n",
    "        array of depth differences, avg depth differences and std   \n",
    "    \"\"\"\n",
    "    depth_diffs = []\n",
    "    answers = []\n",
    "    for idx, row in df.iterrows():\n",
    "        depth0 = row[\"actual_depth_0\"]\n",
    "        depth1 = row[\"actual_depth_1\"]\n",
    "        \n",
    "        diff = depth0-depth1\n",
    "        if diff < 0:\n",
    "            answer = row['stimulus_0']\n",
    "        else:\n",
    "            answer = row['stimulus_1']\n",
    "        answers.append(answer)\n",
    "        depth_diffs.append(abs(depth0-depth1))\n",
    "    \n",
    "    depth_diffs = np.array(depth_diffs)\n",
    "    \n",
    "    return depth_diffs, np.mean(depth_diffs) ,np.std(depth_diffs),answers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_depth_diffs = depth_differences(zscored_data)\n",
    "\n",
    "# add depth difference column to dataframe \n",
    "zscored_data.insert(18, \"depth_difference\", all_depth_diffs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscored_data.insert(19, \"kinect_answer\", all_depth_diffs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discrim_125 = zscored_data.loc[zscored_data['duration'] == 125]\n",
    "final_discrim_250 = zscored_data.loc[zscored_data['duration'] == 250]\n",
    "final_discrim_1000 = zscored_data.loc[zscored_data['duration'] == 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest = ''\n",
    "\n",
    "final_discrim_125.to_csv(dest + '/' + 'final_discrim_125.csv')\n",
    "final_discrim_250.to_csv(dest + '/' + 'final_discrim_250.csv')\n",
    "final_discrim_1000.to_csv(dest + '/' + 'final_discrim_1000.csv')\n",
    "final_discrim.to_csv(dest + '/' + 'final_discrim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_accuracy = get_accuracy(final_discrim)\n",
    "overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_discrimination_stats(df):\n",
    "    '''\n",
    "    Individual discrimination performance and RT \n",
    "    '''\n",
    "    all_stim0 = df.stimulus_0.unique()\n",
    "    \n",
    "    stimuli_stats = {}\n",
    "    for stim0 in all_stim0:\n",
    "        stim0_df = df.loc[df['stimulus_0'] == stim0]\n",
    "        other_stim = stim0_df.stimulus_1.unique()[0]\n",
    "        stim1_df = df.loc[df['stimulus_1'] == stim0]\n",
    "        # df for a specific discrimination trial (collapsed on stim presentation order)\n",
    "        stim_df = pd.concat([stim0_df, stim1_df], ignore_index=True)\n",
    "        stim_125_df = stim_df[stim_df['duration'] == 125.0]\n",
    "        stim_250_df = stim_df[stim_df['duration'] == 250.0]\n",
    "        stim_1000_df = stim_df[stim_df['duration'] == 1000.0] \n",
    "                \n",
    "        stim_depthdiff = stim_df['depth_difference'][0]\n",
    "        \n",
    "        stim0_depth = stim_df['actual_depth_0'][0]\n",
    "        stim1_depth = stim_df['actual_depth_1'][0]\n",
    "        stim_depthbin = np.mean(np.array([stim0_depth,stim1_depth]))\n",
    "        \n",
    "        kinect_answer = stim0_df.kinect_answer.unique()[0]\n",
    "        \n",
    "        try:\n",
    "            stim_acc_125 = get_accuracy(stim_125_df)\n",
    "            stim_acc_250 = get_accuracy(stim_250_df)\n",
    "            stim_acc_1000 = get_accuracy(stim_1000_df)\n",
    "\n",
    "            stim_RT_125 = get_RT(stim_125_df)\n",
    "            stim_RT_250 = get_RT(stim_250_df)\n",
    "            stim_RT_1000 = get_RT(stim_1000_df)\n",
    "\n",
    "            stimuli_stats[stim0] = {'stimulus_1': other_stim,\n",
    "                                    'accuracy_125': stim_acc_125,\n",
    "                                    'RT_125': stim_RT_125,\n",
    "                                    'accuracy_250': stim_acc_250,\n",
    "                                    'RT_250': stim_RT_250,\n",
    "                                    'accuracy_1000': stim_acc_1000,\n",
    "                                    'RT_1000': stim_RT_1000,\n",
    "                                    'avg_depth': stim_depthbin,\n",
    "                                    'depthdifference': stim_depthdiff, \n",
    "                                    'kinect_answer': kinect_answer}\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return stimuli_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_discrim_performance = individual_discrimination_stats(zscored_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_discrim_performance['depth_discrimination_stimuli/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5-target.png']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Z-scored TAC Verbal Judgement Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ''\n",
    "\n",
    "# x data\n",
    "with open(p + 'X_250.npy' , 'rb') as f:\n",
    "    n_TAC_X_250 = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(p + 'X_1000.npy' , 'rb') as f:\n",
    "    n_TAC_X_1000 = np.load(f, allow_pickle=True)\n",
    "# y data \n",
    "with open(p + 'final_y_250.npy' , 'rb') as f:\n",
    "    n_TAC_final_y_250 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "with open(p + 'final_y_1000.npy' , 'rb') as f:\n",
    "    n_TAC_final_y_1000= np.load(f, allow_pickle=True)\n",
    "\n",
    "# std data\n",
    "with open(p + 'std_250.npy' , 'rb') as f:\n",
    "    n_TAC_std_250 = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(p + 'std_1000.npy' , 'rb') as f:\n",
    "    n_TAC_std_1000 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "# ste data\n",
    "with open(p + 'ste_250.npy' , 'rb') as f:\n",
    "    n_TAC_ste_250 = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(p + 'ste_1000.npy' , 'rb') as f:\n",
    "    n_TAC_ste_1000 = np.load(f, allow_pickle=True)\n",
    "\n",
    "# stim data\n",
    "with open(p + 'final_stim_250.npy' , 'rb') as f:\n",
    "    n_TAC_final_stim_250 = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(p + 'final_stim_1000.npy' , 'rb') as f:\n",
    "    n_TAC_final_stim_1000 = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code depth discrimination correctness by the verbal judgement data \n",
    "\n",
    "Create new 'ground truth' answer based on verbal judgement data\n",
    "\n",
    "Task: Respond which image's target was closer to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_VE_answerkey_250 = {} # corresponding answer key for discrimination trials  \n",
    "\n",
    "for key in all_discrim_performance.keys():\n",
    "    targetimg0 = key.split('/')[-1]\n",
    "    folder0 = targetimg0[:-11]\n",
    "    depth_dur_path0 = 'depth_duration_stimuli/' + folder0 + '/' + targetimg0\n",
    "    idx0 = np.where(n_TAC_final_stim_250 == depth_dur_path0)[0][0]\n",
    "    avg_estim_stim0 = n_TAC_final_y_250[idx0]\n",
    "    std0 = n_TAC_std_250[idx0]\n",
    "    ste0 = n_TAC_ste_250[idx0]\n",
    "    \n",
    "    targetimg1 = all_discrim_performance[key]['stimulus_1'].split('/')[-1]\n",
    "    folder1 = targetimg1[:-11]\n",
    "    depth_dur_path1 = 'depth_duration_stimuli/' + folder1 + '/' + targetimg1\n",
    "    idx1= np.where(n_TAC_final_stim_250 == depth_dur_path1)[0][0]\n",
    "    avg_estim_stim1 = n_TAC_final_y_250[idx1]\n",
    "    std1 = n_TAC_std_250[idx1]\n",
    "    ste1 = n_TAC_ste_250[idx1]\n",
    "    \n",
    "    kinect_answer = all_discrim_performance[key]['kinect_answer'].split('/')[-1]\n",
    "\n",
    "    if avg_estim_stim0 < avg_estim_stim1:\n",
    "        # Which target is CLOSER to you?\n",
    "        answer = targetimg0\n",
    "    if avg_estim_stim0 == avg_estim_stim1:\n",
    "        print(targetimg0, targetimg1)\n",
    "    if avg_estim_stim0 > avg_estim_stim1:\n",
    "        answer = targetimg1\n",
    "\n",
    "    n_VE_answerkey_250[key] = {'stimulus_1': targetimg1,\n",
    "                             'stimulus_0_avg_estim': avg_estim_stim0,\n",
    "                             'stimulus_1_avg_estim': avg_estim_stim1,\n",
    "                             'answer': answer,\n",
    "                             'std0': std0,\n",
    "                             'std1': std1,\n",
    "                             'kinect_answer': kinect_answer}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(n_VE_answerkey_250), n_VE_answerkey_250['depth_discrimination_stimuli/002118_2014-06-25_20-32-08_260595134347_rgbf000078-resize_1/002118_2014-06-25_20-32-08_260595134347_rgbf000078-resize_1-target.png']\n",
    "# n_VE_answerkey_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_VE_answerkey_1000 = {} # corresponding answer key for discrimination trials  \n",
    "\n",
    "for key in all_discrim_performance.keys():\n",
    "    targetimg0 = key.split('/')[-1]\n",
    "    folder0 = targetimg0[:-11]\n",
    "    depth_dur_path0 = 'depth_duration_stimuli/' + folder0 + '/' + targetimg0\n",
    "    idx0 = np.where(n_TAC_final_stim_1000 == depth_dur_path0)[0][0]\n",
    "    avg_estim_stim0 = n_TAC_final_y_1000[idx0]\n",
    "    std0 = n_TAC_std_1000[idx0]\n",
    "\n",
    "    targetimg1 = all_discrim_performance[key]['stimulus_1'].split('/')[-1]\n",
    "    folder1 = targetimg1[:-11]\n",
    "    depth_dur_path1 = 'depth_duration_stimuli/' + folder1 + '/' + targetimg1\n",
    "    idx1= np.where(n_TAC_final_stim_1000 == depth_dur_path1)[0][0]\n",
    "    avg_estim_stim1 = n_TAC_final_y_1000[idx1]\n",
    "    std1 = n_TAC_std_1000[idx1]\n",
    "    \n",
    "    if avg_estim_stim0 < avg_estim_stim1:\n",
    "        # Which target is CLOSER to you?\n",
    "        answer = targetimg0\n",
    "    if avg_estim_stim0 == avg_estim_stim1:\n",
    "        print(targetimg0, targetimg1)\n",
    "    if avg_estim_stim0 > avg_estim_stim1:\n",
    "        answer = targetimg1\n",
    "    \n",
    "    kinect_answer = all_discrim_performance[key]['kinect_answer'].split('/')[-1]\n",
    "\n",
    "\n",
    "\n",
    "    n_VE_answerkey_1000[key] = {'stimulus_1': targetimg1,\n",
    "                             'stimulus_0_avg_estim': avg_estim_stim0,\n",
    "                             'stimulus_1_avg_estim': avg_estim_stim1,\n",
    "                             'answer': answer,\n",
    "                             'std0': std0,\n",
    "                             'std1': std1,\n",
    "                             'kinect_answer': kinect_answer}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(n_VE_answerkey_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save answer keys as pickle file \n",
    "\n",
    "# dest = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/depth_discrimination/TAC_discrim_datafiles/matched_discrim_data/n_VE_answerkey_1000.pickle'\n",
    "# with open(dest, 'wb') as handle:\n",
    "#     pickle.dump(n_VE_answerkey_1000, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dest = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/depth_discrimination/TAC_discrim_datafiles/matched_discrim_data/n_VE_answerkey_250.pickle'\n",
    "# with open(dest, 'wb') as handle:\n",
    "#     pickle.dump(n_VE_answerkey_250, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VE Coded Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VE_accuracy(stim0, df, answerkey):\n",
    "    '''\n",
    "    Accuracy based on the verbal judgement data \n",
    "    '''\n",
    "    \n",
    "    \"\"\"\n",
    "    args:\n",
    "        df  \n",
    "    returns:\n",
    "        proportion of correct responses, count of correct responses, count of total trials  \n",
    "    \"\"\"\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    count_total = 0\n",
    "    count_missed = 0\n",
    "    \n",
    "    VE_correct_answer = answerkey[stim0]['answer']\n",
    "    kinect_correct_answer = answerkey[stim0]['kinect_answer']\n",
    "    \n",
    "    for idx, row in df.iterrows(): \n",
    "        choice = row[\"discrim_choice\"]\n",
    "        count_total += 1\n",
    "        if choice == 0.0:\n",
    "            image_choice = row[\"stimulus_0\"]\n",
    "                \n",
    "        if choice == 1.0:\n",
    "            image_choice = row[\"stimulus_1\"]\n",
    "                \n",
    "        if choice == 2.0:\n",
    "            count_missed += 1\n",
    "            \n",
    "        if choice == 3.0:\n",
    "            count_missed += 1\n",
    "        \n",
    "        try:\n",
    "            if image_choice.split('/')[-1] == VE_correct_answer:\n",
    "                count_correct += 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "#     standardError = (0.5*(1-0.5))/count_total\n",
    "    p = count_correct/count_total\n",
    "    standardError = np.sqrt((p*(1-p))/count_total)\n",
    "    \n",
    "    if VE_correct_answer == kinect_correct_answer:\n",
    "        return count_correct/count_total, count_correct, count_total, count_missed, standardError, 'pos'\n",
    "    else:\n",
    "        return count_correct/count_total, count_correct, count_total, count_missed, standardError, 'neg'\n",
    "\n",
    "    \n",
    "def main_VE_accuracy(df, answerkey_125, answerkey_250, answerkey_1000):\n",
    "    all_stim0 = df.stimulus_0.unique()\n",
    "    \n",
    "    stimuli_stats = {}\n",
    "    for stim0 in all_stim0:\n",
    "        try:\n",
    "            # dataframe for stimulus 0\n",
    "            stim0_df = df.loc[df['stimulus_0'] == stim0]\n",
    "            # name of stimulus 1\n",
    "            other_stim = stim0_df.stimulus_1.unique()[0]\n",
    "            # dataframe where stimulus 0 is presented SECOND (same trial)\n",
    "            stim1_df = df.loc[df['stimulus_1'] == stim0]\n",
    "\n",
    "            # df for a specific discrimination trial (collapsed on stim presentation order)\n",
    "            stim_df = pd.concat([stim0_df, stim1_df], ignore_index=True)\n",
    "            stim_125_df = stim_df[stim_df['duration'] == 125.0]\n",
    "            stim_250_df = stim_df[stim_df['duration'] == 250.0]\n",
    "            stim_1000_df = stim_df[stim_df['duration'] == 1000.0] \n",
    "\n",
    "            stim0_depth = stim_df['actual_depth_0'][0]\n",
    "            stim1_depth = stim_df['actual_depth_1'][0]\n",
    "            stim_depthbin = np.mean(np.array([stim0_depth,stim1_depth]))\n",
    "\n",
    "            stim_acc_125 = VE_accuracy(stim0, stim_125_df, answerkey_125)\n",
    "            stim_acc_250 = VE_accuracy(stim0, stim_250_df, answerkey_250)\n",
    "            stim_acc_1000 = VE_accuracy(stim0, stim_1000_df, answerkey_1000)\n",
    "\n",
    "            # difference between verbal judgements divided by joint variance \n",
    "            # abs(VE1-VE2)/sqrt(stda^2 + std2^2)\n",
    "            std0_125 = answerkey_125[stim0]['std0']\n",
    "            std1_125 = answerkey_125[stim0]['std1']\n",
    "            joint_variance_125 = np.sqrt(std0_125**2 + std1_125**2)\n",
    "            JV_regressor_125 = abs(answerkey_125[stim0]['stimulus_0_avg_estim'] - answerkey_125[stim0]['stimulus_1_avg_estim'])/joint_variance_125\n",
    "\n",
    "            std0_250 = answerkey_250[stim0]['std0']\n",
    "            std1_250 = answerkey_250[stim0]['std1']\n",
    "            joint_variance_250 = np.sqrt(std0_250**2 + std1_250**2)\n",
    "            JV_regressor_250 = abs(answerkey_250[stim0]['stimulus_0_avg_estim'] - answerkey_250[stim0]['stimulus_1_avg_estim'])/joint_variance_250\n",
    "\n",
    "            std0_1000 = answerkey_1000[stim0]['std0']\n",
    "            std1_1000 = answerkey_1000[stim0]['std1']\n",
    "            joint_variance_1000 = np.sqrt(std0_1000**2 + std1_1000**2)\n",
    "            JV_regressor_1000 = abs(answerkey_1000[stim0]['stimulus_0_avg_estim'] - answerkey_1000[stim0]['stimulus_1_avg_estim'])/joint_variance_1000\n",
    "            \n",
    "            if stim_acc_125[-1] == 'pos':\n",
    "                VE_depthdifference_125 = abs(answerkey_125[stim0]['stimulus_0_avg_estim'] - answerkey_125[stim0]['stimulus_1_avg_estim'])\n",
    "            else:\n",
    "                VE_depthdifference_125 = -(abs(answerkey_125[stim0]['stimulus_0_avg_estim'] - answerkey_125[stim0]['stimulus_1_avg_estim']))\n",
    "            \n",
    "            if stim_acc_250[-1] == 'pos':\n",
    "                VE_depthdifference_250 = abs(answerkey_250[stim0]['stimulus_0_avg_estim'] - answerkey_250[stim0]['stimulus_1_avg_estim'])\n",
    "            else:\n",
    "                VE_depthdifference_250 = -(abs(answerkey_250[stim0]['stimulus_0_avg_estim'] - answerkey_250[stim0]['stimulus_1_avg_estim']))\n",
    "            \n",
    "            if stim_acc_1000[-1] == 'pos':\n",
    "                VE_depthdifference_1000 = abs(answerkey_1000[stim0]['stimulus_0_avg_estim'] - answerkey_1000[stim0]['stimulus_1_avg_estim'])\n",
    "            else:\n",
    "                VE_depthdifference_1000 = -(abs(answerkey_1000[stim0]['stimulus_0_avg_estim'] - answerkey_1000[stim0]['stimulus_1_avg_estim']))\n",
    "            \n",
    "            stimuli_stats[stim0] = {'stimulus_1': other_stim,\n",
    "                                    'accuracy_125': stim_acc_125\n",
    "                                    'accuracy_250': stim_acc_250,\n",
    "                                    'accuracy_1000': stim_acc_1000,\n",
    "                                    'avg_depth': stim_depthbin,\n",
    "                                    'VE_depthdifference_125': VE_depthdifference_125, \n",
    "                                    'VE_depthdifference_250': VE_depthdifference_250, \n",
    "                                    'VE_depthdifference_1000': VE_depthdifference_1000,\n",
    "                                    'RT_125': get_RT(stim_125_df),\n",
    "                                    'RT_250': get_RT(stim_250_df),\n",
    "                                    'RT_1000': get_RT(stim_1000_df),\n",
    "                                    'JV_regressor_125': JV_regressor_125,\n",
    "                                    'JV_regressor_250': JV_regressor_250,\n",
    "                                    'JV_regressor_1000': JV_regressor_1000,\n",
    "                                    'kinect_answer_125': answerkey_125[stim0]['kinect_answer'],\n",
    "                                    'kinect_answer_250': answerkey_250[stim0]['kinect_answer'],\n",
    "                                    'kinect_answer_1000': answerkey_1000[stim0]['kinect_answer']}\n",
    "            \n",
    "\n",
    "        except:\n",
    "            print(stim0)\n",
    "\n",
    "    return stimuli_stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_all_VE_discrim_performance = main_VE_accuracy(zscored_data, n_VE_answerkey_125, n_VE_answerkey_250, n_VE_answerkey_1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(n_all_VE_discrim_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save performance as pickle file \n",
    "\n",
    "dest = ''\n",
    "with open(dest, 'wb') as handle:\n",
    "    pickle.dump(n_all_VE_discrim_performance, handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_VE_estim_diff_125 = [n_all_VE_discrim_performance[elem]['VE_depthdifference_125'] for elem in n_all_VE_discrim_performance]\n",
    "n_VE_estim_diff_250 = [n_all_VE_discrim_performance[elem]['VE_depthdifference_250'] for elem in n_all_VE_discrim_performance]\n",
    "n_VE_estim_diff_1000 = [n_all_VE_discrim_performance[elem]['VE_depthdifference_1000'] for elem in n_all_VE_discrim_performance]\n",
    "\n",
    "n_VE_accuracy_125 = [n_all_VE_discrim_performance[elem]['accuracy_125'][0] for elem in n_all_VE_discrim_performance]\n",
    "n_VE_accuracy_250 = [n_all_VE_discrim_performance[elem]['accuracy_250'][0] for elem in n_all_VE_discrim_performance]\n",
    "n_VE_accuracy_1000 = [n_all_VE_discrim_performance[elem]['accuracy_1000'][0] for elem in n_all_VE_discrim_performance]\n",
    "\n",
    "n_VE_ste_125 = [n_all_VE_discrim_performance[elem]['accuracy_125'][-2] for elem in n_all_VE_discrim_performance]\n",
    "n_VE_ste_250 = [n_all_VE_discrim_performance[elem]['accuracy_250'][-2] for elem in n_all_VE_discrim_performance]\n",
    "n_VE_ste_1000 = [n_all_VE_discrim_performance[elem]['accuracy_1000'][-2] for elem in n_all_VE_discrim_performance]\n",
    "\n",
    "n_avg_RT_250 = [n_all_VE_discrim_performance[elem]['RT_125'][1] for elem in n_all_VE_discrim_performance]\n",
    "n_avg_RT_250 = [n_all_VE_discrim_performance[elem]['RT_250'][1] for elem in n_all_VE_discrim_performance]\n",
    "n_avg_RT_1000 = [n_all_VE_discrim_performance[elem]['RT_1000'][1] for elem in n_all_VE_discrim_performance]\n",
    "\n",
    "n_avg_RT_ste_250 = [n_all_VE_discrim_performance[elem]['RT_125'][-1] for elem in n_all_VE_discrim_performance]\n",
    "n_avg_RT_ste_250 = [n_all_VE_discrim_performance[elem]['RT_250'][-1] for elem in n_all_VE_discrim_performance]\n",
    "n_avg_RT_ste_1000 = [n_all_VE_discrim_performance[elem]['RT_1000'][-1] for elem in n_all_VE_discrim_performance]\n",
    "\n",
    "n_JV_250 = [n_all_VE_discrim_performance[elem]['JV_regressor_125'] for elem in n_all_VE_discrim_performance]\n",
    "n_JV_250 = [n_all_VE_discrim_performance[elem]['JV_regressor_250'] for elem in n_all_VE_discrim_performance]\n",
    "n_JV_1000 = [n_all_VE_discrim_performance[elem]['JV_regressor_1000'] for elem in n_all_VE_discrim_performance]\n",
    "\n",
    "n_kinectanswer_250 = [n_all_VE_discrim_performance[elem]['kinect_answer_125'] for elem in n_all_VE_discrim_performance]\n",
    "n_kinectanswer_250 = [n_all_VE_discrim_performance[elem]['kinect_answer_250'] for elem in n_all_VE_discrim_performance]\n",
    "n_kinectanswer_1000 = [n_all_VE_discrim_performance[elem]['kinect_answer_1000'] for elem in n_all_VE_discrim_performance]\n",
    "\n",
    "n_stim = [elem for elem in n_all_VE_discrim_performance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Accuracy: Coded by Normalized VE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(n_VE_accuracy_125), np.mean(n_VE_accuracy_250), np.mean(n_VE_accuracy_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('125 ms: Correlation between VE discrimination accuracy and average normalized verbal estimate difference')\n",
    "stats.pearsonr(n_VE_accuracy_125, n_VE_estim_diff_125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('250 ms: Correlation between VE discrimination accuracy and average normalized verbal estimate difference')\n",
    "stats.pearsonr(n_VE_accuracy_250, n_VE_estim_diff_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1000 ms: Correlation between VE discrimination accuracy and average normalized verbal estimate difference')\n",
    "stats.pearsonr(n_VE_accuracy_1000, n_VE_estim_diff_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(np.array(n_VE_estim_diff_125), np.array(n_VE_accuracy_125), 'go', label='250 ms')\n",
    "plt.plot(np.array(n_VE_estim_diff_250), np.array(n_VE_accuracy_250), 'go', label='250 ms')\n",
    "plt.plot(np.array(n_VE_estim_diff_1000), np.array(n_VE_accuracy_1000), 'ro', label='1000 ms')\n",
    "plt.xlabel('Average Normalized Verbal Estimate Difference', fontsize=12)\n",
    "plt.ylabel('VE Discrimination Proportion Correct', fontsize = 12)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.title('VE Discrimination Proportion Correct vs Avg Normalized Verbal Estimate Difference', fontsize = 14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(n_VE_accuracy_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_super(x):\n",
    "    normal = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+-=()\"\n",
    "    super_s = \"ᴬᴮᶜᴰᴱᶠᴳᴴᴵᴶᴷᴸᴹᴺᴼᴾQᴿˢᵀᵁⱽᵂˣʸᶻᵃᵇᶜᵈᵉᶠᵍʰᶦʲᵏˡᵐⁿᵒᵖ۹ʳˢᵗᵘᵛʷˣʸᶻ⁰¹²³⁴⁵⁶⁷⁸⁹⁺⁻⁼⁽⁾\"\n",
    "    res = x.maketrans(''.join(normal), ''.join(super_s))\n",
    "    return x.translate(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[8,8])\n",
    "right_side = ax.spines['right']\n",
    "right_side.set_visible(False)\n",
    "top = ax.spines['top']\n",
    "top.set_visible(False)\n",
    "\n",
    "\n",
    "#run regression\n",
    "VE_X_125 = np.array(n_VE_estim_diff_125).reshape(-1,1)\n",
    "VE_PC_125 = n_VE_accuracy_125\n",
    "VE_PC_reg_125 = LinearRegression().fit(VE_X_125, VE_PC_125)\n",
    "\n",
    "#Generated Predictions\n",
    "PC_predicted_125 = VE_PC_reg_125.predict(VE_X_125)\n",
    "#Plot Our Actual and Predicted Values\n",
    "plt.errorbar(VE_X_125, VE_PC_125,elinewidth=1, ecolor='gray', fmt='or', mfc='white', mec='chocolate', capsize=3, alpha = 0.3, yerr = np.array(n_VE_ste_125));\n",
    "\n",
    "\n",
    "plt.plot(VE_X_125, VE_PC_125, 'o', color='chocolate', alpha = 0.3);\n",
    "plt.plot(VE_X_125, PC_predicted_125,color='chocolate', label = 'm = ' + str(round(VE_PC_reg_125.coef_[0], 3))\n",
    "         + '     r' + get_super('2') + ' = '+ str(round(float(VE_PC_reg_125.score(VE_X_125, VE_PC_125)), 3)))\n",
    "plt.xlabel(\"z-scored Verbal Estimate Difference (m)\", fontsize = 15)\n",
    "plt.ylabel(\"Proportion Correct\", fontsize = 15)\n",
    "\n",
    "plt.plot([-1.5, 1.5], [0.5,0.5], '--', color='black')\n",
    "\n",
    "\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(VE_PC_reg_125.coef_))\n",
    "print(\"b: {0}\".format(VE_PC_reg_125.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", VE_PC_reg_125.score(VE_X_125, VE_PC_125))\n",
    "\n",
    "legend = plt.legend(loc = 4, fontsize=13, labelspacing=1, frameon=False)\n",
    "\n",
    "plt.title('125 ms')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[8,8])\n",
    "right_side = ax.spines['right']\n",
    "right_side.set_visible(False)\n",
    "top = ax.spines['top']\n",
    "top.set_visible(False)\n",
    "\n",
    "\n",
    "#run regression\n",
    "VE_X_250 = np.array(n_VE_estim_diff_250).reshape(-1,1)\n",
    "VE_PC_250 = n_VE_accuracy_250\n",
    "VE_PC_reg_250 = LinearRegression().fit(VE_X_250, VE_PC_250)\n",
    "\n",
    "#Generated Predictions\n",
    "PC_predicted_250 = VE_PC_reg_250.predict(VE_X_250)\n",
    "#Plot Our Actual and Predicted Values\n",
    "plt.errorbar(VE_X_250, VE_PC_250,elinewidth=1, ecolor='gray', fmt='or', mfc='white', mec='chocolate', capsize=3, alpha = 0.3, yerr = np.array(n_VE_ste_250));\n",
    "\n",
    "\n",
    "plt.plot(VE_X_250, VE_PC_250, 'o', color='chocolate', alpha = 0.3);\n",
    "plt.plot(VE_X_250, PC_predicted_250,color='chocolate', label = 'm = ' + str(round(VE_PC_reg_250.coef_[0], 3))\n",
    "         + '     r' + get_super('2') + ' = '+ str(round(float(VE_PC_reg_250.score(VE_X_250, VE_PC_250)), 3)))\n",
    "plt.xlabel(\"z-scored Verbal Estimate Difference (m)\", fontsize = 15)\n",
    "plt.ylabel(\"Proportion Correct\", fontsize = 15)\n",
    "\n",
    "plt.plot([-1.5, 1.5], [0.5,0.5], '--', color='black')\n",
    "\n",
    "\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(VE_PC_reg_250.coef_))\n",
    "print(\"b: {0}\".format(VE_PC_reg_250.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", VE_PC_reg_250.score(VE_X_250, VE_PC_250))\n",
    "\n",
    "legend = plt.legend(loc = 4, fontsize=13, labelspacing=1, frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[8,8])\n",
    "right_side = ax.spines['right']\n",
    "right_side.set_visible(False)\n",
    "top = ax.spines['top']\n",
    "top.set_visible(False)\n",
    "\n",
    "\n",
    "VE_X_1000 = np.array(n_VE_estim_diff_1000).reshape(-1,1)\n",
    "VE_PC_1000 = n_VE_accuracy_1000\n",
    "VE_PC_reg_1000 = LinearRegression().fit(VE_X_1000, VE_PC_1000)\n",
    "\n",
    "#Generated Predictions\n",
    "PC_predicted_1000 = VE_PC_reg_1000.predict(VE_X_1000)\n",
    "#Plot Our Actual and Predicted Values\n",
    "\n",
    "r = np.sqrt(float(VE_PC_reg_1000.score(VE_X_1000, VE_PC_1000)))\n",
    "\n",
    "plt.errorbar(VE_X_1000, VE_PC_1000, elinewidth=1, ecolor='gray', fmt='or', mfc='white', mec='darkviolet', capsize=3, alpha = 0.3, yerr = np.array(n_VE_ste_1000));\n",
    "plt.plot(VE_X_1000, VE_PC_1000, 'o', color='darkviolet', alpha = 0.3);\n",
    "plt.plot(VE_X_1000, PC_predicted_1000,color='darkviolet', label = 'm = ' + str(round(VE_PC_reg_1000.coef_[0], 3))\n",
    "         + '     r' + get_super('2') + ' = '+ str(round(float(VE_PC_reg_1000.score(VE_X_1000, VE_PC_1000)), 3)))\n",
    "\n",
    "legend = plt.legend(loc = 4, fontsize=13, labelspacing=1, frameon=False)\n",
    "\n",
    "plt.plot([-1.5, 1.5], [0.5,0.5], '--', color='black')\n",
    "\n",
    "# plt.plot(VE_X_1000[2], VE_PC_1000[2], 'o', color='black', alpha=1)\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(VE_PC_reg_1000.coef_))\n",
    "print(\"b: {0}\".format(VE_PC_reg_1000.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", VE_PC_reg_1000.score(VE_X_1000, VE_PC_1000))\n",
    "\n",
    "plt.xlabel(\"z-scored Verbal Estimate Difference (m)\", fontsize = 15)\n",
    "plt.ylabel(\"Proportion Correct\", fontsize = 15)\n",
    "\n",
    "\n",
    "# plt.title(\"1000 ms: Accuracy\", fontsize = 20)\n",
    "\n",
    "# plt.xticks(np.arange(-1,1.2, 0.2))\n",
    "# plt.yticks(np.arange(-1,1.2, 0.2))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(n_VE_accuracy_250, n_VE_accuracy_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[7,7])\n",
    "plt.xlabel('Proportion Correct @ 250 ms', fontsize=15)\n",
    "plt.ylabel('Proportion Correct @ 1000 ms', fontsize=15)\n",
    "plt.plot(n_VE_accuracy_250,n_VE_accuracy_1000, 'o' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[8,8])\n",
    "right_side = ax.spines['right']\n",
    "right_side.set_visible(False)\n",
    "top = ax.spines['top']\n",
    "top.set_visible(False)\n",
    "\n",
    "#run regression\n",
    "X_125 = np.array(np.abs(n_VE_estim_diff_125).reshape(-1,1))\n",
    "y_125 = n_avg_RT_125\n",
    "reg_125 = LinearRegression().fit(X_125, y_125)\n",
    "\n",
    "#Generated Predictions\n",
    "y_predicted_125 = reg_125.predict(X_125)\n",
    "#Plot Our Actual and Predicted Values\n",
    "plt.errorbar(X_125, y_125,elinewidth=1, ecolor='gray',fmt='or', mfc='white', mec='chocolate', capsize=3, alpha = 0.3, yerr = np.array(n_avg_RT_ste_125));\n",
    "\n",
    "plt.plot(X_125, y_125, 's', color='chocolate', alpha = 0.3);\n",
    "plt.plot(X_125, y_predicted_125,color='chocolate', label = 'y = ' + str(round(reg_125.coef_[0], 3))\n",
    "        +'x +'+str(round(reg_125.intercept_, 3))  + '     r' + get_super('2') + ' = '+ str(round(float(reg_125.score(X_125, y_125)), 3)))\n",
    "\n",
    "plt.xlabel(\"z-scored Verbal Estimate Difference (m)\", fontsize = 15)\n",
    "plt.ylabel(\"RT (ms)\", fontsize = 15)\n",
    "\n",
    "\n",
    "legend = plt.legend(loc = 4, fontsize=13, labelspacing=1, frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.title('125 ms', fontsize=18)\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(reg_250.coef_))\n",
    "print(\"b: {0}\".format(reg_250.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", reg_250.score(X_250, y_250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[8,8])\n",
    "right_side = ax.spines['right']\n",
    "right_side.set_visible(False)\n",
    "top = ax.spines['top']\n",
    "top.set_visible(False)\n",
    "\n",
    "#run regression\n",
    "X_250 = np.array(np.abs(n_VE_estim_diff_250).reshape(-1,1))\n",
    "y_250 = n_avg_RT_250\n",
    "reg_250 = LinearRegression().fit(X_250, y_250)\n",
    "\n",
    "#Generated Predictions\n",
    "y_predicted_250 = reg_250.predict(X_250)\n",
    "#Plot Our Actual and Predicted Values\n",
    "plt.errorbar(X_250, y_250,elinewidth=1, ecolor='gray',fmt='or', mfc='white', mec='chocolate', capsize=3, alpha = 0.3, yerr = np.array(n_avg_RT_ste_250));\n",
    "\n",
    "plt.plot(X_250, y_250, 's', color='chocolate', alpha = 0.3);\n",
    "plt.plot(X_250, y_predicted_250,color='chocolate', label = 'y = ' + str(round(reg_250.coef_[0], 3))\n",
    "        +'x +'+str(round(reg_250.intercept_, 3))  + '     r' + get_super('2') + ' = '+ str(round(float(reg_250.score(X_250, y_250)), 3)))\n",
    "\n",
    "plt.xlabel(\"z-scored Verbal Estimate Difference (m)\", fontsize = 15)\n",
    "plt.ylabel(\"RT (ms)\", fontsize = 15)\n",
    "\n",
    "\n",
    "legend = plt.legend(loc = 4, fontsize=13, labelspacing=1, frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.xticks(np.arange(0.15,1.2, 0.2))\n",
    "# plt.yticks(np.arange(6.7,7.3, .1))\n",
    "\n",
    "plt.title('250 ms', fontsize=18)\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(reg_250.coef_))\n",
    "print(\"b: {0}\".format(reg_250.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", reg_250.score(X_250, y_250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(n_VE_estim_diff_1000, n_avg_RT_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[8,8])\n",
    "right_side = ax.spines['right']\n",
    "right_side.set_visible(False)\n",
    "top = ax.spines['top']\n",
    "top.set_visible(False)\n",
    "\n",
    "#run regression\n",
    "X_1000 = np.array(np.abs(n_VE_estim_diff_1000).reshape(-1,1))\n",
    "y_1000 = n_avg_RT_1000\n",
    "reg_1000 = LinearRegression().fit(X_1000, y_1000)\n",
    "\n",
    "#Generated Predictions\n",
    "y_predicted_1000 = reg_1000.predict(X_1000)\n",
    "#Plot Our Actual and Predicted Values\n",
    "\n",
    "plt.errorbar(X_1000, y_1000, elinewidth=1, ecolor='gray', fmt='or', mfc='white', mec='chocolate', capsize=3, alpha = 0.3, yerr = np.array(n_avg_RT_ste_1000));\n",
    "\n",
    "r = np.sqrt(float(reg_1000.score(X_1000, y_1000)))\n",
    "\n",
    "plt.plot(X_1000, y_1000, 's', color='darkviolet', alpha = 0.3);\n",
    "plt.plot(X_1000, y_predicted_1000,color='darkviolet', label = 'y = ' + str(round(reg_1000.coef_[0], 3))\n",
    "        +'x +'+str(round(reg_1000.intercept_, 3))  + '     r = '+ str(round(float(r), 3)))\n",
    "\n",
    "plt.xlabel(\"Average Normalized Verbal Estimate Difference (m)\", fontsize = 15)\n",
    "plt.ylabel(\"RT (ms)\", fontsize = 15)\n",
    "\n",
    "\n",
    "legend = plt.legend(loc = 4, fontsize=13, labelspacing=1, frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.yticks(np.arange(6.7,7.3, .1))\n",
    "\n",
    "plt.title('1000 ms', fontsize=18)\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(reg_1000.coef_))\n",
    "print(\"b: {0}\".format(reg_1000.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", reg_1000.score(X_1000, y_1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor: Difference between verbal judgements divided by joint variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('125 ms: Correlation between VE discrimination accuracy and difference between normalized verbal judgements divided by joint variance ')\n",
    "stats.pearsonr(n_VE_accuracy_125, n_JV_125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('250 ms: Correlation between VE discrimination accuracy and difference between normalized verbal judgements divided by joint variance ')\n",
    "stats.pearsonr(n_VE_accuracy_250, n_JV_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1000 ms: Correlation between VE discrimination accuracy and difference between verbal judgements divided by joint variance ')\n",
    "stats.pearsonr(n_VE_accuracy_1000, n_JV_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[7,7])\n",
    "right_side = ax.spines['right']\n",
    "right_side.set_visible(False)\n",
    "top = ax.spines['top']\n",
    "top.set_visible(False)\n",
    "\n",
    "#run regression\n",
    "JV_X_125 = np.array(n_JV_125).reshape(-1,1)\n",
    "VE_PC_125 = n_VE_accuracy_125\n",
    "JV_PC_reg_125 = LinearRegression().fit(JV_X_125, VE_PC_125)\n",
    "\n",
    "#Generated Predictions\n",
    "JV_PC_predicted_125 = JV_PC_reg_250.predict(JV_X_125)\n",
    "#Plot Our Actual and Predicted Values\n",
    "\n",
    "plt.errorbar(JV_X_125, VE_PC_125, elinewidth=1, ecolor='gray', fmt='or', mfc='white', mec='chocolate', capsize=3, alpha = 0.3, yerr = np.array(n_VE_ste_250));\n",
    "plt.plot(JV_X_125, VE_PC_125, 'o', color='chocolate', alpha = 0.3);\n",
    "plt.plot(JV_X_125, JV_PC_predicted_125,color='chocolate', label = 'm = ' + str(round(JV_PC_reg_125.coef_[0], 3))\n",
    "         + '     r-squared = ' + str(round(float(JV_PC_reg_125.score(JV_X_125, VE_PC_125)), 3)))\n",
    "# plt.xlabel(\"Diff b/w normalized verbal judgements divided by joint variance\", fontsize = 15)\n",
    "plt.xlabel(\"Normalized Verbal Estimate Difference divided by Joint Variance\", fontsize = 15)\n",
    "\n",
    "plt.ylabel(\"Proportion Correct\", fontsize = 15)\n",
    "\n",
    "plt.ylim((0,1.05))\n",
    "plt.xticks(np.arange(0,1.8, 0.2))\n",
    "plt.yticks(np.arange(0,1.2, 0.2))\n",
    "plt.margins(x=0.02)\n",
    "\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(JV_PC_reg_125.coef_))\n",
    "print(\"b: {0}\".format(JV_PC_reg_125.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", JV_PC_reg_125.score(JV_X_125, VE_PC_125))\n",
    "\n",
    "# plt.plot([-0.6, 0.9], [0.5,0.5], '--', color='black')\n",
    "\n",
    "\n",
    "legend = plt.legend(loc = 4, fontsize=13, labelspacing=1, frameon=False)\n",
    "\n",
    "# plt.plot(JV_X_250[10], VE_PC_250[10], 'o', color='black', alpha=1)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[7,7])\n",
    "right_side = ax.spines['right']\n",
    "right_side.set_visible(False)\n",
    "top = ax.spines['top']\n",
    "top.set_visible(False)\n",
    "\n",
    "#run regression\n",
    "JV_X_250 = np.array(n_JV_250).reshape(-1,1)\n",
    "VE_PC_250 = n_VE_accuracy_250\n",
    "JV_PC_reg_250 = LinearRegression().fit(JV_X_250, VE_PC_250)\n",
    "\n",
    "#Generated Predictions\n",
    "JV_PC_predicted_250 = JV_PC_reg_250.predict(JV_X_250)\n",
    "#Plot Our Actual and Predicted Values\n",
    "\n",
    "plt.errorbar(JV_X_250, VE_PC_250, elinewidth=1, ecolor='gray', fmt='or', mfc='white', mec='chocolate', capsize=3, alpha = 0.3, yerr = np.array(n_VE_ste_250));\n",
    "plt.plot(JV_X_250, VE_PC_250, 'o', color='chocolate', alpha = 0.3);\n",
    "plt.plot(JV_X_250, JV_PC_predicted_250,color='chocolate', label = 'm = ' + str(round(JV_PC_reg_250.coef_[0], 3))\n",
    "         + '     r-squared = ' + str(round(float(JV_PC_reg_250.score(JV_X_250, VE_PC_250)), 3)))\n",
    "# plt.xlabel(\"Diff b/w normalized verbal judgements divided by joint variance\", fontsize = 15)\n",
    "plt.xlabel(\"Normalized Verbal Estimate Difference divided by Joint Variance\", fontsize = 15)\n",
    "\n",
    "plt.ylabel(\"Proportion Correct\", fontsize = 15)\n",
    "\n",
    "plt.ylim((0,1.05))\n",
    "plt.xticks(np.arange(0,1.8, 0.2))\n",
    "plt.yticks(np.arange(0,1.2, 0.2))\n",
    "plt.margins(x=0.02)\n",
    "\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(JV_PC_reg_250.coef_))\n",
    "print(\"b: {0}\".format(JV_PC_reg_250.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", JV_PC_reg_250.score(JV_X_250, VE_PC_250))\n",
    "\n",
    "# plt.plot([-0.6, 0.9], [0.5,0.5], '--', color='black')\n",
    "\n",
    "\n",
    "legend = plt.legend(loc = 4, fontsize=13, labelspacing=1, frameon=False)\n",
    "\n",
    "# plt.plot(JV_X_250[10], VE_PC_250[10], 'o', color='black', alpha=1)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[7,7])\n",
    "right_side = ax.spines['right']\n",
    "right_side.set_visible(False)\n",
    "top = ax.spines['top']\n",
    "top.set_visible(False)\n",
    "\n",
    "#run regression\n",
    "JV_X_1000 = np.array(n_JV_1000).reshape(-1,1)\n",
    "VE_PC_1000 = n_VE_accuracy_1000\n",
    "JV_PC_reg_1000 = LinearRegression().fit(JV_X_1000, VE_PC_1000)\n",
    "\n",
    "#Generated Predictions\n",
    "JV_PC_predicted_1000 = JV_PC_reg_1000.predict(JV_X_1000)\n",
    "#Plot Our Actual and Predicted Values\n",
    "\n",
    "plt.errorbar(JV_X_1000, VE_PC_1000, elinewidth=1, ecolor='gray', fmt='or', mfc='white', mec='darkviolet', capsize=3, alpha = 0.3, yerr = np.array(n_VE_ste_250));\n",
    "plt.plot(JV_X_1000, VE_PC_1000, 'o', color='darkviolet', alpha = 0.3); \n",
    "plt.plot(JV_X_1000, JV_PC_predicted_1000,color='darkviolet', label = 'm = ' + str(round(JV_PC_reg_1000.coef_[0], 3))\n",
    "         + '     r-squared = ' + str(round(float(JV_PC_reg_1000.score(JV_X_1000, VE_PC_1000)), 3)))\n",
    "# plt.xlabel(\"Diff b/w normalized verbal judgements divided by joint variance\", fontsize = 15)\n",
    "plt.xlabel(\"Normalized Verbal Estimate Difference divided by Joint Variance\", fontsize = 15)\n",
    "\n",
    "plt.ylabel(\"Proportion Correct\", fontsize = 15)\n",
    "\n",
    "plt.xticks(np.arange(0,1.8, 0.2))\n",
    "plt.yticks(np.arange(0,1.2, 0.2))\n",
    "plt.margins(x=0.02)\n",
    "\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(JV_PC_reg_1000.coef_))\n",
    "print(\"b: {0}\".format(JV_PC_reg_1000.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", JV_PC_reg_1000.score(JV_X_1000, VE_PC_1000))\n",
    "\n",
    "# plt.plot([-0.6, 0.9], [0.5,0.5], '--', color='black')\n",
    "\n",
    "\n",
    "legend = plt.legend(loc = 4, fontsize=13, labelspacing=1, frameon=False)\n",
    "\n",
    "\n",
    "# plt.plot(JV_X_1000[10], VE_PC_1000[10], 'o', color='black', alpha=3)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.pearsonr(n_JV_250, n_JV_1000))\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.xlabel('@ 250 ms', fontsize=15)\n",
    "plt.ylabel('@ 1000 ms', fontsize=15)\n",
    "plt.plot(n_JV_250,n_JV_1000, 'o' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('125 ms: Correlation between average discrimination RT and difference between verbal judgements divided by joint variance ')\n",
    "stats.pearsonr(n_avg_RT_125, n_JV_125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('250 ms: Correlation between average discrimination RT and difference between verbal judgements divided by joint variance ')\n",
    "stats.pearsonr(n_avg_RT_250, n_JV_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1000 ms: Correlation between average discrimination RT and difference between verbal judgements divided by joint variance ')\n",
    "stats.pearsonr(n_avg_RT_1000, n_JV_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run regression\n",
    "plt.figure(figsize=[8,6])\n",
    "JV_X_125 = np.array(n_JV_125).reshape(-1,1)\n",
    "VE_PC_125 = n_avg_RT_125\n",
    "JV_PC_reg_125 = LinearRegression().fit(JV_X_125, VE_PC_125)\n",
    "\n",
    "#Generated Predictions\n",
    "JV_PC_predicted_125 = JV_PC_reg_125.predict(JV_X_125)\n",
    "#Plot Our Actual and Predicted Values\n",
    "\n",
    "plt.errorbar(JV_X_125, VE_PC_125, elinewidth=1, ecolor='gray', fmt='or', mfc='white', mec='chocolate', capsize=3, alpha = 0.3, yerr = np.array(n_avg_RT_ste_125));\n",
    "\n",
    "\n",
    "plt.plot(JV_X_125, VE_PC_125, 'o', color='chocolate', alpha = 0.3);\n",
    "plt.plot(JV_X_125, JV_PC_predicted_125,color='chocolate', label = 'm = ' + str(round(JV_PC_reg_125.coef_[0], 3))\n",
    "         + '     r-squared = ' + str(round(float(JV_PC_reg_125.score(JV_X_125, VE_PC_125)), 3)))\n",
    "plt.xlabel(\"Difference between normalized verbal judgements divided by joint variance\", fontsize = 15)\n",
    "plt.ylabel(\"RT (ms)\", fontsize = 15)\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(JV_PC_reg_125.coef_))\n",
    "print(\"b: {0}\".format(JV_PC_reg_125.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", JV_PC_reg_125.score(JV_X_125, VE_PC_125))\n",
    "\n",
    "\n",
    "legend = plt.legend(loc = 4, fontsize = 13, borderpad = 0.6, labelspacing = 1)\n",
    "legend.get_frame().set_facecolor('lightgray')\n",
    "\n",
    "plt.title(\"125 ms: RT\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run regression\n",
    "plt.figure(figsize=[8,6])\n",
    "JV_X_250 = np.array(n_JV_250).reshape(-1,1)\n",
    "VE_PC_250 = n_avg_RT_250\n",
    "JV_PC_reg_250 = LinearRegression().fit(JV_X_250, VE_PC_250)\n",
    "\n",
    "#Generated Predictions\n",
    "JV_PC_predicted_250 = JV_PC_reg_250.predict(JV_X_250)\n",
    "#Plot Our Actual and Predicted Values\n",
    "\n",
    "plt.errorbar(JV_X_250, VE_PC_250, elinewidth=1, ecolor='gray', fmt='or', mfc='white', mec='chocolate', capsize=3, alpha = 0.3, yerr = np.array(n_avg_RT_ste_1000));\n",
    "\n",
    "\n",
    "plt.plot(JV_X_250, VE_PC_250, 'o', color='chocolate', alpha = 0.3);\n",
    "plt.plot(JV_X_250, JV_PC_predicted_250,color='chocolate', label = 'm = ' + str(round(JV_PC_reg_250.coef_[0], 3))\n",
    "         + '     r-squared = ' + str(round(float(JV_PC_reg_250.score(JV_X_250, VE_PC_250)), 3)))\n",
    "plt.xlabel(\"Difference between normalized verbal judgements divided by joint variance\", fontsize = 15)\n",
    "plt.ylabel(\"RT (ms)\", fontsize = 15)\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(JV_PC_reg_250.coef_))\n",
    "print(\"b: {0}\".format(JV_PC_reg_250.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", JV_PC_reg_250.score(JV_X_250, VE_PC_250))\n",
    "\n",
    "\n",
    "legend = plt.legend(loc = 4, fontsize = 13, borderpad = 0.6, labelspacing = 1)\n",
    "legend.get_frame().set_facecolor('lightgray')\n",
    "\n",
    "plt.title(\"250 ms: RT\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run regression\n",
    "plt.figure(figsize=[8,6])\n",
    "JV_X_1000 = np.array(n_JV_1000).reshape(-1,1)\n",
    "VE_PC_1000 = n_avg_RT_1000\n",
    "JV_PC_reg_1000 = LinearRegression().fit(JV_X_1000, VE_PC_1000)\n",
    "\n",
    "#Generated Predictions\n",
    "JV_PC_predicted_1000 = JV_PC_reg_1000.predict(JV_X_1000)\n",
    "#Plot Our Actual and Predicted Values\n",
    "\n",
    "plt.errorbar(JV_X_1000, VE_PC_1000, elinewidth=1, ecolor='gray', fmt='or', mfc='white', mec='chocolate', capsize=3, alpha = 0.3, yerr = np.array(n_avg_RT_ste_1000));\n",
    "\n",
    "plt.plot(JV_X_1000, VE_PC_1000, 'o', color='darkviolet', alpha = 0.3);\n",
    "plt.plot(JV_X_1000, JV_PC_predicted_1000,color='darkviolet', label = 'm = ' + str(round(JV_PC_reg_1000.coef_[0], 3))\n",
    "         + '     r-squared = ' + str(round(float(JV_PC_reg_1000.score(JV_X_1000, VE_PC_1000)), 3)))\n",
    "plt.xlabel(\"Difference between normalized verbal judgements divided by joint variance\", fontsize = 15)\n",
    "plt.ylabel(\"RT (ms)\", fontsize = 15)\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(JV_PC_reg_1000.coef_))\n",
    "print(\"b: {0}\".format(JV_PC_reg_1000.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", JV_PC_reg_1000.score(JV_X_1000, VE_PC_1000))\n",
    "\n",
    "\n",
    "legend = plt.legend(loc = 4, fontsize = 13, borderpad = 0.6, labelspacing = 1)\n",
    "legend.get_frame().set_facecolor('lightgray')\n",
    "\n",
    "plt.title(\"1000 ms: RT\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrim x VE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir = ''\n",
    "rawVE_125_data = pd.read_csv(_dir + '/raw_125_data.csv')\n",
    "rawVE_250_data = pd.read_csv(_dir + '/raw_250_data.csv')\n",
    "rawVE_1000_data = pd.read_csv(_dir + '/raw_1000_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetimgs_discrim = []\n",
    "\n",
    "for stim in n_stim:\n",
    "    targetimgs_discrim.append(stim.split('/')[1] + '/' + stim.split('/')[-1])\n",
    "    \n",
    "targetimgs_discrim[0], len(targetimgs_discrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "VE_250_Performance = {}\n",
    "# loop through discrimination target images\n",
    "for im0 in targetimgs_discrim:\n",
    "    performance = []\n",
    "    # loop through all participants\n",
    "    for subjID in rawVE_250_data.subjID.unique():\n",
    "        # filter to just the subjects data df\n",
    "        subjdf_250 = rawVE_250_data.loc[rawVE_250_data['subjID'] == subjID]\n",
    "        # extract stimuli and depth estimate list for the participant\n",
    "        particip_VE_stim_250 = list(subjdf_250['stimulus'])\n",
    "        particip_VE_y_250 = list(subjdf_250['depth_estimate'])\n",
    "        # convert stim to just the targetimg part of the path\n",
    "        particip_VE_targetimgs_250 = []\n",
    "        for stim in particip_VE_stim_250:\n",
    "            particip_VE_targetimgs_250.append(stim.split('/')[1] + '/' + stim.split('/')[-1])\n",
    "        \n",
    "        try:\n",
    "            # index for that image in the verbal judgement data \n",
    "            im0_VE_index_250 = particip_VE_targetimgs_250.index(im0)\n",
    "            # depth estimate for this image at 250 ms\n",
    "            im0_particip_VE_y_250 = particip_VE_y_250[im0_VE_index_250]\n",
    "\n",
    "            key = 'depth_discrimination_stimuli/' + im0\n",
    "\n",
    "            im1 = n_all_VE_discrim_performance[key]['stimulus_1'][29:]\n",
    "            # index for that image in the verbal judgement data \n",
    "            im1_VE_index_250 = particip_VE_targetimgs_250.index(im1)\n",
    "            # depth estimate for this image at 250 ms\n",
    "            im1_particip_VE_y_250 = particip_VE_y_250[im1_VE_index_250]\n",
    "            \n",
    "            if im0_particip_VE_y_250 < im1_particip_VE_y_250:\n",
    "                p_ans = im0.split('/')[-1]\n",
    "            else:\n",
    "                p_ans = im1.split('/')[-1]\n",
    "            try:\n",
    "                answerkey_answer = n_VE_answerkey_250[key]['answer']\n",
    "            except:\n",
    "                answerkey_answer = n_VE_answerkey_250['depth_discrimination_stimuli/' + im1]['answer']\n",
    "            if p_ans == answerkey_answer:\n",
    "                trial_acc = 0 # CORRECT\n",
    "                performance.append(trial_acc)\n",
    "            else:\n",
    "                trial_acc = 1 # INCORRECT\n",
    "                performance.append(trial_acc)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    VE_250_Performance[im0] = performance\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VE_250_Performance['001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5-target.png'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/Users/prachi/Documents/depth_duration/depth_discrimination/TAC_discrim_datafiles/matched_discrim_data/'\n",
    "\n",
    "with open(p + 'VE_250_Performance.pickle', 'wb') as handle:\n",
    "    pickle.dump(VE_250_Performance, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to count the number of occurrences\n",
    "def countX(lst, x):\n",
    "    return lst.count(x)\n",
    "\n",
    "VE_250_PC = {}\n",
    "\n",
    "for key in VE_250_Performance:\n",
    "    performance = VE_250_Performance[key]\n",
    "    correct_count = countX(performance, 0)\n",
    "    incorrect_count = countX(performance, 1)\n",
    "    total = len(performance)\n",
    "    pc = correct_count/total\n",
    "    VE_250_PC['depth_discrimination_stimuli/' + key] = pc\n",
    "    \n",
    "VE_250_PC['depth_discrimination_stimuli/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5-target.png'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Discrim_VE_PC_250 = {}\n",
    "all_stim_250 = []\n",
    "all_VE_PC_250 = []\n",
    "all_Discrim_VE_PC_250 = []\n",
    "\n",
    "for key in VE_250_PC:\n",
    "    im_VE_PC = VE_250_PC[key]\n",
    "    im_Discrim_PC = n_all_VE_discrim_performance[key]['accuracy_250'][0]\n",
    "    Discrim_VE_PC_250[key] = [im_VE_PC, im_Discrim_PC]\n",
    "    \n",
    "    all_stim_250.append(key)\n",
    "    all_VE_PC_250.append(im_VE_PC)\n",
    "    all_Discrim_VE_PC_250.append(im_Discrim_PC)\n",
    "    \n",
    "Discrim_VE_PC_250['depth_discrimination_stimuli/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5-target.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stim_250[0], all_VE_PC_250[0], all_Discrim_VE_PC_250[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_stim_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[8,8])\n",
    "\n",
    "right_side = ax.spines[\"right\"]\n",
    "right_side.set_visible(False)\n",
    "top = ax.spines['top']\n",
    "top.set_visible(False)\n",
    "\n",
    "#run regression\n",
    "X_250 = np.array(all_VE_PC_250).reshape(-1,1)\n",
    "y_250 = np.array(all_Discrim_VE_PC_250)\n",
    "reg_250 = LinearRegression().fit(X_250, y_250)\n",
    "\n",
    "#Generated Predictions\n",
    "y_predicted_250 = reg_250.predict(X_250)\n",
    "#Plot Our Actual and Predicted Values\n",
    "plt.plot(X_250, y_250, 'o', color='chocolate', alpha = 0.7);\n",
    "\n",
    "r = np.sqrt(reg_250.score(X_250, y_250))\n",
    "\n",
    "plt.plot(X_250,y_predicted_250,color='chocolate', label = 'y = ' + str(round(reg_250.coef_[0], 3)) + 'x + ' +  str(round(reg_250.intercept_, 3))\n",
    "         + '     r' + '=' +  str(round(float(r), 3)))\n",
    "\n",
    "x_perfacc = np.arange(0.1 ,1.1, 0.1)\n",
    "plt.plot(x_perfacc, x_perfacc, color = 'black',linestyle='--')  # solid\n",
    "\n",
    "legend = plt.legend(loc = 0, fontsize = 13, labelspacing = 1, frameon=False)\n",
    "\n",
    "plt.xlabel('Verbal Estimate Percent Correct', fontsize=15)\n",
    "plt.ylabel('Discrimination Percent Correct', fontsize=15)\n",
    "\n",
    "plt.xticks(np.arange(0.1, 1.1, 0.1))\n",
    "plt.yticks(np.arange(0.1, 1.1, 0.1))\n",
    "\n",
    "\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(reg_250.coef_))\n",
    "print(\"b: {0}\".format(reg_250.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", reg_250.score(X_250, y_250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "VE_1000_Performance = {}\n",
    "# loop through discrimination target images\n",
    "for im0 in targetimgs_discrim:\n",
    "    performance = []\n",
    "    # loop through all participants\n",
    "    for subjID in rawVE_1000_data.subjID.unique():\n",
    "        # filter to just the subjects data df\n",
    "        subjdf_1000 = rawVE_1000_data.loc[rawVE_1000_data['subjID'] == subjID]\n",
    "        # extract stimuli and depth estimate list for the participant\n",
    "        particip_VE_stim_1000 = list(subjdf_1000['stimulus'])\n",
    "        particip_VE_y_1000 = list(subjdf_1000['depth_estimate'])\n",
    "        # convert stim to just the targetimg part of the path\n",
    "        particip_VE_targetimgs_1000 = []\n",
    "        for stim in particip_VE_stim_1000:\n",
    "            particip_VE_targetimgs_1000.append(stim.split('/')[1] + '/' + stim.split('/')[-1])\n",
    "        \n",
    "        try:\n",
    "            # index for that image in the verbal judgement data \n",
    "            im0_VE_index_1000 = particip_VE_targetimgs_1000.index(im0)\n",
    "            # depth estimate for this image at 250 ms\n",
    "            im0_particip_VE_y_1000 = particip_VE_y_1000[im0_VE_index_1000]\n",
    "\n",
    "            key = 'depth_discrimination_stimuli/' + im0\n",
    "\n",
    "            im1 = n_all_VE_discrim_performance[key]['stimulus_1'][29:]\n",
    "            # index for that image in the verbal judgement data \n",
    "            im1_VE_index_1000 = particip_VE_targetimgs_1000.index(im1)\n",
    "            # depth estimate for this image at 1000 ms\n",
    "            im1_particip_VE_y_1000 = particip_VE_y_1000[im1_VE_index_1000]\n",
    "            \n",
    "            if im0_particip_VE_y_1000 < im1_particip_VE_y_1000:\n",
    "                p_ans = im0.split('/')[-1]\n",
    "            else:\n",
    "                p_ans = im1.split('/')[-1]\n",
    "            try:\n",
    "                answerkey_answer = n_VE_answerkey_1000[key]['answer']\n",
    "            except:\n",
    "                answerkey_answer = n_VE_answerkey_1000['depth_discrimination_stimuli/' + im1]['answer']\n",
    "            if p_ans == answerkey_answer:\n",
    "                trial_acc = 0 # CORRECT\n",
    "                performance.append(trial_acc)\n",
    "            else:\n",
    "                trial_acc = 1 # INCORRECT\n",
    "                performance.append(trial_acc)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    VE_1000_Performance[im0] = performance\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/Users/prachi/Documents/depth_duration/depth_discrimination/TAC_discrim_datafiles/matched_discrim_data/'\n",
    "with open(p + 'VE_1000_Performance.pickle', 'wb') as handle:\n",
    "    pickle.dump(VE_1000_Performance, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(VE_250_Performance['001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5-target.png'])\n",
    "# VE_1000_Performance['001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5-target.png']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to count the number of occurrences\n",
    "def countX(lst, x):\n",
    "    return lst.count(x)\n",
    "\n",
    "VE_1000_PC = {}\n",
    "\n",
    "for key in VE_1000_Performance:\n",
    "    performance = VE_1000_Performance[key]\n",
    "    correct_count = countX(performance, 0)\n",
    "    incorrect_count = countX(performance, 1)\n",
    "    total = len(performance)\n",
    "    pc = correct_count/total\n",
    "    VE_1000_PC['depth_discrimination_stimuli/' + key] = pc\n",
    "    \n",
    "VE_1000_PC['depth_discrimination_stimuli/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5-target.png'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Discrim_VE_PC_1000 = {}\n",
    "all_stim_1000 = []\n",
    "all_VE_PC_1000 = []\n",
    "all_Discrim_VE_PC_1000 = []\n",
    "\n",
    "for key in VE_1000_PC:\n",
    "    im_VE_PC = VE_1000_PC[key]\n",
    "    im_Discrim_PC = n_all_VE_discrim_performance[key]['accuracy_1000'][0]\n",
    "    Discrim_VE_PC_1000[key] = [im_VE_PC, im_Discrim_PC]\n",
    "    \n",
    "    all_stim_1000.append(key)\n",
    "    all_VE_PC_1000.append(im_VE_PC)\n",
    "    all_Discrim_VE_PC_1000.append(im_Discrim_PC)\n",
    "    \n",
    "Discrim_VE_PC_1000['depth_discrimination_stimuli/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5-target.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stim_1000[0], all_VE_PC_1000[0], all_Discrim_VE_PC_1000[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[8,8])\n",
    "\n",
    "right_side = ax.spines[\"right\"]\n",
    "right_side.set_visible(False)\n",
    "top = ax.spines['top']\n",
    "top.set_visible(False)\n",
    "\n",
    "#run regression\n",
    "X_1000 = np.array(all_VE_PC_1000).reshape(-1,1)\n",
    "y_1000 = np.array(all_Discrim_VE_PC_1000)\n",
    "reg_1000 = LinearRegression().fit(X_1000, y_1000)\n",
    "\n",
    "#Generated Predictions\n",
    "y_predicted_1000 = reg_1000.predict(X_1000)\n",
    "#Plot Our Actual and Predicted Values\n",
    "plt.plot(X_1000, y_1000, 'o', color='darkviolet', alpha = 0.7);\n",
    "\n",
    "r = np.sqrt(reg_1000.score(X_1000, y_1000))\n",
    "\n",
    "plt.plot(X_1000,y_predicted_1000,color='darkviolet', label = 'y = ' + str(round(reg_1000.coef_[0], 3)) + 'x + ' +  str(round(reg_1000.intercept_, 3))\n",
    "         + '     r' + '=' +  str(round(float(r), 3)))\n",
    "\n",
    "x_perfacc = np.arange(0.1 ,1.1, 0.1)\n",
    "plt.plot(x_perfacc, x_perfacc, color = 'black',linestyle='--')  # solid\n",
    "\n",
    "legend = plt.legend(loc = 0, fontsize = 13, labelspacing = 1, frameon=False)\n",
    "\n",
    "plt.xlabel('Verbal Estimate Percent Correct', fontsize=15)\n",
    "plt.ylabel('Discrimination Percent Correct', fontsize=15)\n",
    "\n",
    "plt.xticks(np.arange(0.1, 1.1, 0.1))\n",
    "plt.yticks(np.arange(0.1, 1.1, 0.1))\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(reg_1000.coef_))\n",
    "print(\"b: {0}\".format(reg_1000.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", reg_1000.score(X_1000, y_1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
