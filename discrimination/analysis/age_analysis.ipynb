{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matched Discrimination Age Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "import scipy.stats as stats\n",
    "from scipy.stats import sem \n",
    "from mpl_toolkits.mplot3d import Axes3D # <--- This is important for 3d plotting \n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data if pipeline has already been run through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _dir = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/depth_discrimination'\n",
    "_dir = '/Users/pmahableshwarkar/Documents/Depth_Project/depth_discrimination'\n",
    "# raw_dir = _dir + '/data/finalVEMatched/raw/'\n",
    "zs_dir = _dir + '/data/finalDiscrimMatched/z_scored_RT/'\n",
    "\n",
    "# final_data = pd.read_csv (raw_dir + '/raw_discrim.csv')\n",
    "zscored_data = pd.read_csv (zs_dir + 'final_discrim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = []\n",
    "for subj in zscored_data.subjID.unique():\n",
    "    subj_age = zscored_data.loc[zscored_data['subjID']==subj].age.unique()[0]\n",
    "    if subj_age < 18:\n",
    "        print(subj_age)\n",
    "        pass\n",
    "    else:\n",
    "        if subj_age > 1900:\n",
    "            subj_age = 2022 - subj_age\n",
    "            \n",
    "    ages.append(subj_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.05150214592275, 11.233453635288466)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(ages)), np.std(np.array(ages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation with Verbal Report data \n",
    "\n",
    "- For each discrimination trial, get the average difference between the verbal report for those scenes \n",
    "- Correlate that with the proportion correct for that trial \n",
    "- Is there better accuracy for scenes that have a larger percevied depth difference (using the verbal report data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Discriminations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(df):\n",
    "    \"\"\"\n",
    "    * ACCURACY BASED ON THE KINECT\n",
    "    args:\n",
    "        df  \n",
    "    returns:\n",
    "        proportion of correct responses, count of correct responses, count of total trials  \n",
    "    \"\"\"\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    count_total = 0\n",
    "    count_missed = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        choice = row[\"discrim_choice\"]\n",
    "        if choice == 2.0:\n",
    "            count_missed += 1\n",
    "        else:    \n",
    "            count_total += 1\n",
    "            depth0 = row[\"actual_depth_0\"]\n",
    "            depth1 = row[\"actual_depth_1\"]\n",
    "            if depth0 < depth1:\n",
    "                correct_choice = 0\n",
    "            if depth0 > depth1:\n",
    "                correct_choice = 1\n",
    "            if depth0 == depth1:\n",
    "                # case where depths are equal \n",
    "                correct_choice = None\n",
    "            if choice == correct_choice:\n",
    "                count_correct += 1\n",
    "    \n",
    "    return count_correct/count_total, count_correct, count_total, count_missed\n",
    "\n",
    "def get_RT(df):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        df  \n",
    "    returns:\n",
    "        array of RTs, avg RT and std   \n",
    "    \"\"\"\n",
    "    list_RTs = []\n",
    "    for idx, row in df.iterrows():   \n",
    "        list_RTs.append(row[\"trial_RT\"])\n",
    "    \n",
    "    list_RTs = np.array(list_RTs)\n",
    "    \n",
    "    return list_RTs, np.mean(list_RTs) ,np.std(list_RTs), stats.sem(list_RTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discrim = zscored_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5949060378102724, 21115, 35493, 494)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_accuracy = get_accuracy(final_discrim)\n",
    "overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5953159615993249, 11286, 18958, 216) (0.5932171276998863, 9393, 15834, 277)\n"
     ]
    }
   ],
   "source": [
    "print(get_accuracy(m_zscored_data), get_accuracy(w_zscored_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_discrimination_stats(df):\n",
    "    '''\n",
    "    Individual discrimination performance and RT \n",
    "    '''\n",
    "    all_stim0 = [elem for elem in df.stimulus_0.unique() if type(elem) == str]\n",
    "\n",
    "    stimuli_stats = {}\n",
    "    for stim0 in all_stim0:\n",
    "        stim0_df = df.loc[df['stimulus_0'] == stim0]\n",
    "        other_stim = stim0_df.stimulus_1.unique()[0]\n",
    "        stim1_df = df.loc[df['stimulus_1'] == stim0]\n",
    "        # df for a specific discrimination trial (collapsed on stim presentation order)\n",
    "        stim_df = pd.concat([stim0_df, stim1_df], ignore_index=True)\n",
    "        stim_125_df = stim_df[stim_df['duration'] == 125.0]\n",
    "        stim_250_df = stim_df[stim_df['duration'] == 250.0]\n",
    "        stim_1000_df = stim_df[stim_df['duration'] == 1000.0] \n",
    "                \n",
    "        stim_depthdiff = stim_df['depth_difference'][0]\n",
    "        \n",
    "        stim0_depth = stim_df['actual_depth_0'][0]\n",
    "        stim1_depth = stim_df['actual_depth_1'][0]\n",
    "        stim_depthbin = np.mean(np.array([stim0_depth,stim1_depth]))\n",
    "        \n",
    "        kinect_answer = stim0_df.kinect_answer.unique()[0]\n",
    "        \n",
    "        try:\n",
    "            stim_acc_125 = get_accuracy(stim_125_df)\n",
    "            stim_acc_250 = get_accuracy(stim_250_df)\n",
    "            stim_acc_1000 = get_accuracy(stim_1000_df)\n",
    "\n",
    "            stim_RT_125 = get_RT(stim_125_df)\n",
    "            stim_RT_250 = get_RT(stim_250_df)\n",
    "            stim_RT_1000 = get_RT(stim_1000_df)\n",
    "\n",
    "            stimuli_stats[stim0] = {'stimulus_1': other_stim,\n",
    "                                    'accuracy_125': stim_acc_125,\n",
    "                                    'RT_125': stim_RT_125,\n",
    "                                    'accuracy_250': stim_acc_250,\n",
    "                                    'RT_250': stim_RT_250,\n",
    "                                    'accuracy_1000': stim_acc_1000,\n",
    "                                    'RT_1000': stim_RT_1000,\n",
    "                                    'avg_depth': stim_depthbin,\n",
    "                                    'depthdifference': stim_depthdiff, \n",
    "                                    'kinect_answer': kinect_answer}\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return stimuli_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_all_discrim_performance = individual_discrimination_stats(m_zscored_data)\n",
    "w_all_discrim_performance = individual_discrimination_stats(w_zscored_data)\n",
    "all_discrim_performance = individual_discrimination_stats(zscored_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_discrim_performance['depth_discrimination_stimuli/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5/001417_2014-06-19_16-25-36_260595134347_rgbf000115-resize_5-target.png']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Z-scored TAC Verbal Judgement Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/verbal_judgement_analysis/data/finalVEMatched/z_scored/'\n",
    "\n",
    "# x data\n",
    "with open(p + 'X_125.npy' , 'rb') as f:\n",
    "    n_TAC_X_125 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "with open(p + 'X_250.npy' , 'rb') as f:\n",
    "    n_TAC_X_250 = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(p + 'X_1000.npy' , 'rb') as f:\n",
    "    n_TAC_X_1000 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "# y data \n",
    "with open(p + 'z_final_y_125.npy' , 'rb') as f:\n",
    "    n_TAC_final_y_125 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "with open(p + 'z_final_y_250.npy' , 'rb') as f:\n",
    "    n_TAC_final_y_250 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "with open(p + 'z_final_y_1000.npy' , 'rb') as f:\n",
    "    n_TAC_final_y_1000= np.load(f, allow_pickle=True)\n",
    "\n",
    "# std data\n",
    "with open(p + 'z_std_125.npy' , 'rb') as f:\n",
    "    n_TAC_std_125 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "with open(p + 'z_std_250.npy' , 'rb') as f:\n",
    "    n_TAC_std_250 = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(p + 'z_std_1000.npy' , 'rb') as f:\n",
    "    n_TAC_std_1000 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "# ste data\n",
    "with open(p + 'z_ste_125.npy' , 'rb') as f:\n",
    "    n_TAC_ste_125 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "with open(p + 'z_ste_250.npy' , 'rb') as f:\n",
    "    n_TAC_ste_250 = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(p + 'z_ste_1000.npy' , 'rb') as f:\n",
    "    n_TAC_ste_1000 = np.load(f, allow_pickle=True)\n",
    "\n",
    "# stim data\n",
    "with open(p + 'z_final_stim_125.npy' , 'rb') as f:\n",
    "    n_TAC_final_stim_125 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "with open(p + 'z_final_stim_250.npy' , 'rb') as f:\n",
    "    n_TAC_final_stim_250 = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(p + 'z_final_stim_1000.npy' , 'rb') as f:\n",
    "    n_TAC_final_stim_1000 = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code depth discrimination correctness by the verbal judgement data \n",
    "\n",
    "Create new 'ground truth' answer based on verbal judgement data\n",
    "\n",
    "Task: Respond which image's target was closer to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_VE_answerkey_125 = {} # corresponding answer key for discrimination trials  \n",
    "\n",
    "for key in all_discrim_performance.keys():\n",
    "    targetimg0 = key.split('/')[-1]\n",
    "    folder0 = targetimg0[:-11]\n",
    "    depth_dur_path0 = 'depth_duration_stimuli/' + folder0 + '/' + targetimg0\n",
    "    idx0 = np.where(n_TAC_final_stim_125 == depth_dur_path0)[0][0]\n",
    "    avg_estim_stim0 = n_TAC_final_y_125[idx0]\n",
    "    std0 = n_TAC_std_125[idx0]\n",
    "    ste0 = n_TAC_ste_125[idx0]\n",
    "    \n",
    "    targetimg1 = all_discrim_performance[key]['stimulus_1'].split('/')[-1]\n",
    "    folder1 = targetimg1[:-11]\n",
    "    depth_dur_path1 = 'depth_duration_stimuli/' + folder1 + '/' + targetimg1\n",
    "    idx1= np.where(n_TAC_final_stim_125 == depth_dur_path1)[0][0]\n",
    "    avg_estim_stim1 = n_TAC_final_y_125[idx1]\n",
    "    std1 = n_TAC_std_125[idx1]\n",
    "    ste1 = n_TAC_ste_125[idx1]\n",
    "    \n",
    "    kinect_answer = all_discrim_performance[key]['kinect_answer'].split('/')[-1]\n",
    "\n",
    "    if avg_estim_stim0 < avg_estim_stim1:\n",
    "        # Which target is CLOSER to you?\n",
    "        answer = targetimg0\n",
    "    if avg_estim_stim0 == avg_estim_stim1:\n",
    "        print(targetimg0, targetimg1)\n",
    "    if avg_estim_stim0 > avg_estim_stim1:\n",
    "        answer = targetimg1\n",
    "\n",
    "    n_VE_answerkey_125[key] = {'stimulus_1': targetimg1,\n",
    "                             'stimulus_0_avg_estim': avg_estim_stim0,\n",
    "                             'stimulus_1_avg_estim': avg_estim_stim1,\n",
    "                             'answer': answer,\n",
    "                             'std0': std0,\n",
    "                             'std1': std1,\n",
    "                             'kinect_answer': kinect_answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_VE_answerkey_250 = {} # corresponding answer key for discrimination trials  \n",
    "\n",
    "for key in all_discrim_performance.keys():\n",
    "    targetimg0 = key.split('/')[-1]\n",
    "    folder0 = targetimg0[:-11]\n",
    "    depth_dur_path0 = 'depth_duration_stimuli/' + folder0 + '/' + targetimg0\n",
    "    idx0 = np.where(n_TAC_final_stim_250 == depth_dur_path0)[0][0]\n",
    "    avg_estim_stim0 = n_TAC_final_y_250[idx0]\n",
    "    std0 = n_TAC_std_250[idx0]\n",
    "    ste0 = n_TAC_ste_250[idx0]\n",
    "    \n",
    "    targetimg1 = all_discrim_performance[key]['stimulus_1'].split('/')[-1]\n",
    "    folder1 = targetimg1[:-11]\n",
    "    depth_dur_path1 = 'depth_duration_stimuli/' + folder1 + '/' + targetimg1\n",
    "    idx1= np.where(n_TAC_final_stim_250 == depth_dur_path1)[0][0]\n",
    "    avg_estim_stim1 = n_TAC_final_y_250[idx1]\n",
    "    std1 = n_TAC_std_250[idx1]\n",
    "    ste1 = n_TAC_ste_250[idx1]\n",
    "    \n",
    "    kinect_answer = all_discrim_performance[key]['kinect_answer'].split('/')[-1]\n",
    "\n",
    "    if avg_estim_stim0 < avg_estim_stim1:\n",
    "        # Which target is CLOSER to you?\n",
    "        answer = targetimg0\n",
    "    if avg_estim_stim0 == avg_estim_stim1:\n",
    "        print(targetimg0, targetimg1)\n",
    "    if avg_estim_stim0 > avg_estim_stim1:\n",
    "        answer = targetimg1\n",
    "\n",
    "    n_VE_answerkey_250[key] = {'stimulus_1': targetimg1,\n",
    "                             'stimulus_0_avg_estim': avg_estim_stim0,\n",
    "                             'stimulus_1_avg_estim': avg_estim_stim1,\n",
    "                             'answer': answer,\n",
    "                             'std0': std0,\n",
    "                             'std1': std1,\n",
    "                             'kinect_answer': kinect_answer}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_VE_answerkey_1000 = {} # corresponding answer key for discrimination trials  \n",
    "\n",
    "for key in all_discrim_performance.keys():\n",
    "    targetimg0 = key.split('/')[-1]\n",
    "    folder0 = targetimg0[:-11]\n",
    "    depth_dur_path0 = 'depth_duration_stimuli/' + folder0 + '/' + targetimg0\n",
    "    idx0 = np.where(n_TAC_final_stim_1000 == depth_dur_path0)[0][0]\n",
    "    avg_estim_stim0 = n_TAC_final_y_1000[idx0]\n",
    "    std0 = n_TAC_std_1000[idx0]\n",
    "\n",
    "    targetimg1 = all_discrim_performance[key]['stimulus_1'].split('/')[-1]\n",
    "    folder1 = targetimg1[:-11]\n",
    "    depth_dur_path1 = 'depth_duration_stimuli/' + folder1 + '/' + targetimg1\n",
    "    idx1= np.where(n_TAC_final_stim_1000 == depth_dur_path1)[0][0]\n",
    "    avg_estim_stim1 = n_TAC_final_y_1000[idx1]\n",
    "    std1 = n_TAC_std_1000[idx1]\n",
    "    \n",
    "    if avg_estim_stim0 < avg_estim_stim1:\n",
    "        # Which target is CLOSER to you?\n",
    "        answer = targetimg0\n",
    "    if avg_estim_stim0 == avg_estim_stim1:\n",
    "        print(targetimg0, targetimg1)\n",
    "    if avg_estim_stim0 > avg_estim_stim1:\n",
    "        answer = targetimg1\n",
    "    \n",
    "    kinect_answer = all_discrim_performance[key]['kinect_answer'].split('/')[-1]\n",
    "\n",
    "\n",
    "\n",
    "    n_VE_answerkey_1000[key] = {'stimulus_1': targetimg1,\n",
    "                             'stimulus_0_avg_estim': avg_estim_stim0,\n",
    "                             'stimulus_1_avg_estim': avg_estim_stim1,\n",
    "                             'answer': answer,\n",
    "                             'std0': std0,\n",
    "                             'std1': std1,\n",
    "                             'kinect_answer': kinect_answer}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_VE_answerkey_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save answer keys as pickle file \n",
    "\n",
    "# dest = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/depth_discrimination/TAC_discrim_datafiles/matched_discrim_data/n_VE_answerkey_1000.pickle'\n",
    "# with open(dest, 'wb') as handle:\n",
    "#     pickle.dump(n_VE_answerkey_1000, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dest = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/depth_discrimination/TAC_discrim_datafiles/matched_discrim_data/n_VE_answerkey_250.pickle'\n",
    "# with open(dest, 'wb') as handle:\n",
    "#     pickle.dump(n_VE_answerkey_250, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VE Coded Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VE_accuracy(stim0, df, answerkey):\n",
    "    '''\n",
    "    Accuracy based on the verbal judgement data \n",
    "    '''\n",
    "    \n",
    "    \"\"\"\n",
    "    args:\n",
    "        df  \n",
    "    returns:\n",
    "        proportion of correct responses, count of correct responses, count of total trials  \n",
    "    \"\"\"\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    count_total = 0\n",
    "    count_missed = 0\n",
    "    \n",
    "    VE_correct_answer = answerkey[stim0]['answer']\n",
    "    kinect_correct_answer = answerkey[stim0]['kinect_answer']\n",
    "    \n",
    "    for idx, row in df.iterrows(): \n",
    "        choice = row[\"discrim_choice\"]\n",
    "        count_total += 1\n",
    "        if choice == 0.0:\n",
    "            image_choice = row[\"stimulus_0\"]\n",
    "                \n",
    "        if choice == 1.0:\n",
    "            image_choice = row[\"stimulus_1\"]\n",
    "                \n",
    "        if choice == 2.0:\n",
    "            count_missed += 1\n",
    "            \n",
    "        if choice == 3.0:\n",
    "            count_missed += 1\n",
    "        \n",
    "        try:\n",
    "            if image_choice.split('/')[-1] == VE_correct_answer:\n",
    "                count_correct += 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "#     standardError = (0.5*(1-0.5))/count_total\n",
    "    p = count_correct/count_total\n",
    "    standardError = np.sqrt((p*(1-p))/count_total)\n",
    "    \n",
    "    if VE_correct_answer == kinect_correct_answer:\n",
    "        return count_correct/count_total, count_correct, count_total, count_missed, standardError, 'pos'\n",
    "    else:\n",
    "        return count_correct/count_total, count_correct, count_total, count_missed, standardError, 'neg'\n",
    "\n",
    "    \n",
    "def main_VE_accuracy(df, answerkey_125, answerkey_250, answerkey_1000):\n",
    "    all_stim0 = df.stimulus_0.unique()\n",
    "    \n",
    "    stimuli_stats = {}\n",
    "    for stim0 in all_stim0:\n",
    "        try:\n",
    "            # dataframe for stimulus 0\n",
    "            stim0_df = df.loc[df['stimulus_0'] == stim0]\n",
    "            # name of stimulus 1\n",
    "            other_stim = stim0_df.stimulus_1.unique()[0]\n",
    "            # dataframe where stimulus 0 is presented SECOND (same trial)\n",
    "            stim1_df = df.loc[df['stimulus_1'] == stim0]\n",
    "\n",
    "            # df for a specific discrimination trial (collapsed on stim presentation order)\n",
    "            stim_df = pd.concat([stim0_df, stim1_df], ignore_index=True)\n",
    "            stim_125_df = stim_df[stim_df['duration'] == 125.0]\n",
    "            stim_250_df = stim_df[stim_df['duration'] == 250.0]\n",
    "            stim_1000_df = stim_df[stim_df['duration'] == 1000.0] \n",
    "\n",
    "            stim0_depth = stim_df['actual_depth_0'][0]\n",
    "            stim1_depth = stim_df['actual_depth_1'][0]\n",
    "            stim_depthbin = np.mean(np.array([stim0_depth,stim1_depth]))\n",
    "\n",
    "            stim_acc_125 = VE_accuracy(stim0, stim_125_df, answerkey_125)\n",
    "            stim_acc_250 = VE_accuracy(stim0, stim_250_df, answerkey_250)\n",
    "            stim_acc_1000 = VE_accuracy(stim0, stim_1000_df, answerkey_1000)\n",
    "\n",
    "            # difference between verbal judgements divided by joint variance \n",
    "            # abs(VE1-VE2)/sqrt(stda^2 + std2^2)\n",
    "            std0_125 = answerkey_125[stim0]['std0']\n",
    "            std1_125 = answerkey_125[stim0]['std1']\n",
    "            joint_variance_125 = np.sqrt(std0_125**2 + std1_125**2)\n",
    "            JV_regressor_125 = abs(answerkey_125[stim0]['stimulus_0_avg_estim'] - answerkey_125[stim0]['stimulus_1_avg_estim'])/joint_variance_125\n",
    "\n",
    "            std0_250 = answerkey_250[stim0]['std0']\n",
    "            std1_250 = answerkey_250[stim0]['std1']\n",
    "            joint_variance_250 = np.sqrt(std0_250**2 + std1_250**2)\n",
    "            JV_regressor_250 = abs(answerkey_250[stim0]['stimulus_0_avg_estim'] - answerkey_250[stim0]['stimulus_1_avg_estim'])/joint_variance_250\n",
    "\n",
    "            std0_1000 = answerkey_1000[stim0]['std0']\n",
    "            std1_1000 = answerkey_1000[stim0]['std1']\n",
    "            joint_variance_1000 = np.sqrt(std0_1000**2 + std1_1000**2)\n",
    "            JV_regressor_1000 = abs(answerkey_1000[stim0]['stimulus_0_avg_estim'] - answerkey_1000[stim0]['stimulus_1_avg_estim'])/joint_variance_1000\n",
    "            \n",
    "            if stim_acc_125[-1] == 'pos':\n",
    "                VE_depthdifference_125 = abs(answerkey_125[stim0]['stimulus_0_avg_estim'] - answerkey_125[stim0]['stimulus_1_avg_estim'])\n",
    "            else:\n",
    "                VE_depthdifference_125 = -(abs(answerkey_125[stim0]['stimulus_0_avg_estim'] - answerkey_125[stim0]['stimulus_1_avg_estim']))\n",
    "            \n",
    "            if stim_acc_250[-1] == 'pos':\n",
    "                VE_depthdifference_250 = abs(answerkey_250[stim0]['stimulus_0_avg_estim'] - answerkey_250[stim0]['stimulus_1_avg_estim'])\n",
    "            else:\n",
    "                VE_depthdifference_250 = -(abs(answerkey_250[stim0]['stimulus_0_avg_estim'] - answerkey_250[stim0]['stimulus_1_avg_estim']))\n",
    "            \n",
    "            if stim_acc_1000[-1] == 'pos':\n",
    "                VE_depthdifference_1000 = abs(answerkey_1000[stim0]['stimulus_0_avg_estim'] - answerkey_1000[stim0]['stimulus_1_avg_estim'])\n",
    "            else:\n",
    "                VE_depthdifference_1000 = -(abs(answerkey_1000[stim0]['stimulus_0_avg_estim'] - answerkey_1000[stim0]['stimulus_1_avg_estim']))\n",
    "            \n",
    "            stimuli_stats[stim0] = {'stimulus_1': other_stim,\n",
    "                                    'accuracy_125': stim_acc_125,\n",
    "                                    'accuracy_250': stim_acc_250,\n",
    "                                    'accuracy_1000': stim_acc_1000,\n",
    "                                    'avg_depth': stim_depthbin,\n",
    "                                    'VE_depthdifference_125': VE_depthdifference_125, \n",
    "                                    'VE_depthdifference_250': VE_depthdifference_250, \n",
    "                                    'VE_depthdifference_1000': VE_depthdifference_1000,\n",
    "                                    'RT_125': get_RT(stim_125_df),\n",
    "                                    'RT_250': get_RT(stim_250_df),\n",
    "                                    'RT_1000': get_RT(stim_1000_df),\n",
    "                                    'JV_regressor_125': JV_regressor_125,\n",
    "                                    'JV_regressor_250': JV_regressor_250,\n",
    "                                    'JV_regressor_1000': JV_regressor_1000,\n",
    "                                    'kinect_answer_125': answerkey_125[stim0]['kinect_answer'],\n",
    "                                    'kinect_answer_250': answerkey_250[stim0]['kinect_answer'],\n",
    "                                    'kinect_answer_1000': answerkey_1000[stim0]['kinect_answer']}\n",
    "            \n",
    "\n",
    "        except:\n",
    "            print(stim0)\n",
    "\n",
    "    return stimuli_stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
