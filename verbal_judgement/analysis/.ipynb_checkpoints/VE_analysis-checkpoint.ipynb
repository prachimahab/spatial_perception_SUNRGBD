{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matched VE Analysis: Updated Pipeline with Z-Scored Outcome Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineCSVs(datafolder):\n",
    "    \"\"\"\n",
    "    Combine all participant data into one pandas df\n",
    "    OR \n",
    "    Create df for single participant file \n",
    "\n",
    "    exclude: list of subject IDs that should be excluded from the final df \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    exclude = []\n",
    "    \n",
    "    #checks if path is a file\n",
    "    isFile = os.path.isfile(datafolder)\n",
    "\n",
    "    #checks if path is a directory\n",
    "    \n",
    "    isDirectory = os.path.isdir(datafolder)\n",
    "    \n",
    "    if isDirectory == True:\n",
    "        data = []\n",
    "        for filename in os.listdir(datafolder):\n",
    "            if 'csv' in filename:\n",
    "                path = datafolder + \"/\" + filename\n",
    "                df = pd.read_csv(path, index_col=None, header=0)\n",
    "                \n",
    "                # do NOT include subject IDs that have been flagged \n",
    "                subjID = df.subjID.unique()[0]\n",
    "                if subjID not in exclude:\n",
    "                    data.append(df)\n",
    "\n",
    "        input_frame = pd.concat(data, axis=0, ignore_index=True)\n",
    "        \n",
    "    if isFile == True:\n",
    "        if 'csv' in datafolder:\n",
    "            input_frame = pd.read_csv(datafolder, index_col=None, header=0)\n",
    "    \n",
    "    print('Number of participants before cleaning: ', len(input_frame.subjID.unique()))\n",
    "\n",
    " \n",
    "    return input_frame\n",
    "\n",
    "\n",
    "def feet_to_meters(ft):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        ft = float value in feet \n",
    "        \n",
    "    returns:\n",
    "        m = float value converted to meters \n",
    "    \"\"\"\n",
    "    m = ft * 0.3048\n",
    "    return m\n",
    "\n",
    "def getUnitConveredData(datafolder):\n",
    "    input_data = combineCSVs(datafolder) # combine CSVs from all participants \n",
    "    \n",
    "    for idx, row in input_data.iterrows():\n",
    "        unit = row['unitSelection']\n",
    "        # if estimate was made in feet, convert to meters \n",
    "        if unit == 'feet':\n",
    "            estim_ft = row['depth_estimate']\n",
    "            estim_m = feet_to_meters(estim_ft)\n",
    "            # update depth estimates in existing dataframe\n",
    "            input_data.at[idx, 'depth_estimate'] = estim_m\n",
    "            # update units in existing dataframe\n",
    "#             input_data.at[idx, 'unitSelection'] = 'meters'\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "def cleanAgeResponses(datafolder):\n",
    "    input_data = getUnitConveredData(datafolder)\n",
    "    \n",
    "    for idx, row in input_data.iterrows():\n",
    "        age = row['age']\n",
    "        # if year of birth was given, convert to age\n",
    "        if age > 1900:\n",
    "            actual_age = 2022-age\n",
    "            # update age in existing dataframe\n",
    "            input_data.at[idx, 'age'] = actual_age\n",
    "\n",
    "    return input_data    \n",
    "\n",
    "def catchTrial_cleaning(df, correct_requirement, catch_stimuli, sequence_count):\n",
    "    '''\n",
    "    Participants complete 8 catch trials total to ensure that they are doing the task.\n",
    "    If less than 6/8 catch trials are correct, the participant is excluded.  \n",
    "    '''\n",
    "    \n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    remove = []\n",
    "    subj_sequence = {}\n",
    "    df2_list = []\n",
    "    \n",
    "    for subj in all_subjIDs:\n",
    "#         print(subj)\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning\n",
    "        subj_sequence[subj] = subj_df.sequenceName.unique()[0]\n",
    "        \n",
    "        count_correct = 0\n",
    "        for idx, row in subj_df.iterrows():\n",
    "            stim = row['stimulus']\n",
    "            if type(stim) == str:\n",
    "                if stim.split('/')[1] in catch_stimuli:\n",
    "                    ####### VERSION WHERE CATCH TRIALS ARE ATTENTION CHECK: IMAGE HAS NO TARGET\n",
    "#                     print(row['depth_estimate'])\n",
    "#                     print(row['stimulus'])\n",
    "                    if row[\"depth_estimate\"] == 0:\n",
    "                        count_correct += 1\n",
    "\n",
    "                    # remove catch trial \n",
    "                    cleaned_subj_df.drop([idx], inplace=True)\n",
    "\n",
    "        if count_correct < correct_requirement:\n",
    "            remove.append(subj)\n",
    "        else:\n",
    "            sequence_count[subj_df.sequenceName.unique()[0]] += 1\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "    print(\"Number of participants that did not pass the catch trial check:\", len(remove))\n",
    "    print(\"Participants that were removed:\",remove)\n",
    "\n",
    "    for index, row in df2.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df2.drop(index, inplace=True)\n",
    "    \n",
    "    return df2\n",
    "    \n",
    "\n",
    "def removeMissedTrials(input_data, num_trials):\n",
    "    \"\"\"\n",
    "    Participants were told that if they missed a trial, to respond '0'.\n",
    "    This function removes those trials, and keeps track of:\n",
    "    (1) How many missed trials per participant\n",
    "    (2) Number of missed trials per duration \n",
    "    (3) Number of missed trials per sequence \n",
    "    \"\"\"\n",
    "#     input_data = cleanAgeResponses(datafolder)\n",
    "    \n",
    "    missedTrials_participants = {}\n",
    "    missedTrials_durations = {}\n",
    "    missedTrials_sequences = {}\n",
    "    \n",
    "    \n",
    "    for idx, row in input_data.iterrows():\n",
    "        estimate = row['depth_estimate']\n",
    "        # do catch trial check FIRST\n",
    "        # then have the missing trial function \n",
    "        if estimate == 0.0:\n",
    "            subjID = row['subjID']\n",
    "            duration = row['duration']\n",
    "            sequenceName = row['sequenceName']\n",
    "            \n",
    "            if subjID not in missedTrials_participants:\n",
    "                missedTrials_participants[subjID] = 1\n",
    "            else:\n",
    "                missedTrials_participants[subjID] += 1\n",
    "\n",
    "            if duration not in missedTrials_durations:\n",
    "                missedTrials_durations[duration] = 1\n",
    "            else:\n",
    "                missedTrials_durations[duration] += 1\n",
    "            \n",
    "            if sequenceName not in missedTrials_sequences:\n",
    "                missedTrials_sequences[sequenceName] = 1\n",
    "            else:\n",
    "                missedTrials_sequences[sequenceName] += 1\n",
    "            \n",
    "#             print(subjID, duration, sequenceName)\n",
    "            \n",
    "            # remove trials with depth estimate = 0 \n",
    "            input_data.drop(idx, inplace=True)\n",
    "    \n",
    "    # remove participants data if the participant's missed trial count is 10% or more of num_trials\n",
    "    threshold = math.floor(num_trials * 0.1)\n",
    "#     print(\"Missing Trial Count Threshold: \", threshold)\n",
    "    remove_ids = []\n",
    "    for key in missedTrials_participants:\n",
    "        if missedTrials_participants[key] >= threshold:\n",
    "            remove_ids.append(key)\n",
    "    print(\"Number of participants with 10% or more missed trials: \", len(remove_ids))\n",
    "\n",
    "    for index, row in input_data.iterrows():\n",
    "        if row['subjID'] in remove_ids:\n",
    "            input_data.drop(index, inplace=True)\n",
    "\n",
    "    # Note if a particular participant, duration, or sequence has maximum missing trials\n",
    "    # ** If the participant had no missed trials, then ID will not show up in dict \n",
    "#     print(\"Missed Trials\")\n",
    "#     print(missedTrials_participants)\n",
    "#     print(missedTrials_durations)\n",
    "#     print(missedTrials_sequences)\n",
    "\n",
    "    \n",
    "    return input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_path = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/verbal_judgement_analysis/matched_sequences/9_2022/125ms'\n",
    "sequences_count_dict = {}\n",
    "for seq in os.listdir(sequences_path):\n",
    "    if 'json' in seq:\n",
    "        sequences_count_dict['jsons/'+seq] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_catch_stim = ['000375_2014-06-08_11-17-29_260595134347_rgbf000133-resize_2',\n",
    "                  '000569_2014-06-09_22-51-47_260595134347_rgbf000141-resize_3',\n",
    "                  '000787_2014-06-08_22-33-53_260595134347_rgbf000175-resize_1',\n",
    "                  '002072_2014-06-24_21-48-06_260595134347_rgbf000115-resize_0',\n",
    "                  '001170_2014-06-17_15-43-44_260595134347_rgbf000096-resize_6',\n",
    "                  '001222_2014-06-17_16-24-06_260595134347_rgbf000073-resize_0',\n",
    "                  '001498_2014-06-19_17-45-14_260595134347_rgbf000129-resize_4',\n",
    "                  '001540_2014-06-20_17-01-05_260595134347_rgbf000086-resize_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_trial_cleaned_data = catchTrial_cleaning(path, 6, all_catch_stim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_trial_cleaned_data = removeMissedTrials(catch_trial_cleaned_data, num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Unit Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_unitconversion_data = combineCSVs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids_pre_cleaning = pre_unitconversion_data.subjID.unique()\n",
    "subj_units = {}\n",
    "meters_count = 0\n",
    "feet_count = 0\n",
    "for subj in subject_ids_pre_cleaning:\n",
    "    subj_df = pre_unitconversion_data.loc[pre_unitconversion_data['subjID'] == subj]\n",
    "    unit = subj_df.unitSelection.unique()\n",
    "    subj_units[subj] = unit[0]\n",
    "    if unit[0] == \"feet\":\n",
    "        feet_count += 1\n",
    "    if unit[0] == \"meters\":\n",
    "        meters_count += 1\n",
    "    \n",
    "meters_count, feet_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this version, the RT exclusion criterion is the same for all participants [1000 ms, 10000 ms]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RT_Cleaning(df, outlier_range, num_trials):\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    df2_list = []\n",
    "    for subj in all_subjIDs:\n",
    "        count = 0\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning \n",
    "        # calculate subject's average trial RT \n",
    "        average_trial_RT = subj_df[\"trial_RT\"].mean()\n",
    "        std_trial_RT = subj_df[\"trial_RT\"].std()\n",
    "\n",
    "        for idx, row in subj_df.iterrows():\n",
    "            RT = row[\"trial_RT\"]\n",
    "            if RT < outlier_range[0]: # outlier\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "            if RT > outlier_range[1]:\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "                \n",
    "        threshold = math.floor(num_trials * 0.1)\n",
    "        if count >= threshold:\n",
    "            remove.append(subj)\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "            \n",
    "    print(\"Number of Participants with 10% or more trials outside their RT range: \", len(remove))\n",
    "    \n",
    "    for index, row in df2.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df2.drop(index, inplace=True)\n",
    "                \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_cleaned_data = RT_Cleaning(missed_trial_cleaned_data, [250, 10000], num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeatResponses_Cleaning(df):\n",
    "    \"\"\"\n",
    "    Some participants gave'junk data' - same number repeated for many trials \n",
    "    Count the frequency of unique responses entered by the participant. \n",
    "    If you look at the maximum number of repeats and/or the number of unique responses / 48 per participant you will find our vandals.\n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    max_repeats_distribution = []\n",
    "    num_unique_responses_distribution = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        # ideally, the max repeats and num_unique_responses should be ~ 48 since there are 48 imgs at each depth bin \n",
    "        count_depth_estimates = subj_df['depth_estimate'].value_counts()\n",
    "        num_unique_responses = len(count_depth_estimates)\n",
    "        num_unique_responses_distribution.append(num_unique_responses)\n",
    "        max_repeats = count_depth_estimates.max()\n",
    "        max_repeats_distribution.append(max_repeats)\n",
    "        if num_unique_responses < 6:\n",
    "            remove.append(subj)\n",
    "    \n",
    "    avg_max_repeats = np.array(max_repeats_distribution).mean()\n",
    "    std_max_repeats = np.array(max_repeats_distribution).std()\n",
    "    \n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        count_depth_estimates = subj_df['depth_estimate'].value_counts()\n",
    "        max_repeats = count_depth_estimates.max()\n",
    "\n",
    "        outlierrange = [avg_max_repeats - (3*std_max_repeats), avg_max_repeats + (3*std_max_repeats)]\n",
    "        if max_repeats < outlierrange[0]:\n",
    "            if subj not in remove:\n",
    "                remove.append(subj)\n",
    "        if max_repeats > outlierrange[1]:\n",
    "            if subj not in remove:\n",
    "                remove.append(subj)\n",
    "                \n",
    "    print(\"Number of participants removed: repeat responses: \", len(remove))\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df.drop(index, inplace=True)\n",
    "\n",
    "    \n",
    "    return df, max_repeats_distribution, num_unique_responses_distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_resp_cleaned_data, max_repeats_distrib, num_unique_distrib = repeatResponses_Cleaning(RT_cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat_resp_cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalTrialCountCheck(df, num_trials):\n",
    "    \"\"\"\n",
    "    If more then 10% of a participants data is missing, remove the participant\n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        count_trials = len(subj_df.index)\n",
    "        threshold_trials_remaining = num_trials - math.floor(num_trials * 0.1)\n",
    "\n",
    "        if count_trials <= threshold_trials_remaining:\n",
    "            remove.append(subj)\n",
    "            \n",
    "    print(\"Number of Participants with >= 10% trials removed: \", len(remove))\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df.drop(index, inplace=True)\n",
    "                    \n",
    "    print(\"Number of participants left: \",len(df.subjID.unique()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = finalTrialCountCheck(repeat_resp_cleaned_data, num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subselect subjects that have 156 trials (none removed in outlier cleaning)\n",
    "\n",
    "completeData_subjects = []\n",
    "cD_distribution = []\n",
    "all_subjIDs = cleaned_data.subjID.unique()\n",
    "    \n",
    "for subj in all_subjIDs:\n",
    "    subj_df = cleaned_data.loc[cleaned_data['subjID'] == subj]\n",
    "    count_trials = len(subj_df.index)\n",
    "    cD_distribution.append(count_trials)\n",
    "    if count_trials == 156:\n",
    "        completeData_subjects.append(subj)\n",
    "\n",
    "print('# of subjs with 156 trials: ',len(completeData_subjects))\n",
    "print('Average # of trials: ', int(np.mean(cD_distribution)))\n",
    "\n",
    "plt.hist(cD_distribution, color='gray')\n",
    "# plt.xticks(np.arange(174, 194, 2))\n",
    "plt.title('Distribution of Participant Trial Count')\n",
    "plt.xlabel('Number of Trials')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove participants\n",
    "remove = []\n",
    "for index, row in cleaned_data.iterrows():\n",
    "    if row['subjID'] in remove:\n",
    "        cleaned_data.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = cleaned_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_data.subjID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score Depth Estimates and RT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscored_outcomes(df):\n",
    "    '''\n",
    "    z-score depth estimates and RTs:\n",
    "        for each subj calculate their avg and std \n",
    "        zscored = (estim - subj avg)/subj std\n",
    "    '''\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    df2_list = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        final_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning \n",
    "        # Z-Score depth estimates\n",
    "        average_estim = subj_df[\"depth_estimate\"].mean()\n",
    "        std_estim = subj_df[\"depth_estimate\"].std()\n",
    "        subj_depth_estimates = np.array(list(subj_df[\"depth_estimate\"]))\n",
    "        zscored_subj_depth_estimates = (subj_depth_estimates - average_estim)/std_estim\n",
    "        final_subj_df.replace(subj_depth_estimates, zscored_subj_depth_estimates, inplace=True)\n",
    "        # Z-Score RT\n",
    "        average_RT = subj_df[\"trial_RT\"].mean()\n",
    "        std_RT = subj_df[\"trial_RT\"].std()\n",
    "        subj_RTs = np.array(list(subj_df[\"trial_RT\"]))\n",
    "        zscored_subj_RTs = (subj_RTs - average_RT)/std_RT\n",
    "        final_subj_df.replace(subj_RTs, zscored_subj_RTs, inplace=True)\n",
    "        df2_list.append(final_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)    \n",
    "\n",
    "    return df2\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscored_data = zscored_outcomes(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zscored_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_depth_estimate = final_data['depth_estimate'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_subjects = final_data.subjID.unique()\n",
    "num_feet = 0\n",
    "num_meters = 0\n",
    "feet_subjects = []\n",
    "for subj in final_subjects:\n",
    "    unit = subj_units[subj]\n",
    "    if unit == 'feet':\n",
    "        num_feet += 1\n",
    "        feet_subjects.append(subj)\n",
    "    else:\n",
    "        num_meters += 1\n",
    "\n",
    "num_feet, num_meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ages = final_data['age']\n",
    "plt.figure(figsize = [7,5])\n",
    "plt.xticks(np.arange(20,100, 4) ,fontsize=12)\n",
    "# plt.yticks([])\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title('Distribution of Age', fontsize=16)\n",
    "_, bins, _= plt.hist(all_ages, 50, density=1, alpha=0.5, color='black')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "\n",
    "def subject_pivotTable(data, duration):\n",
    "    \"\"\"\n",
    "    Generate pivot tables from data after cleaning and outlier removal \n",
    "    Organizes data such that for each rounded actual depth the following is calulated:\n",
    "    - average depth estimation\n",
    "    - standard deviation\n",
    "    - standard error \n",
    "    \"\"\"\n",
    "\n",
    "    ###### CREATE DF WITH DATA STATISTICS AFTER OUTLIER REMOVAL ######\n",
    "    avg = pd.pivot_table(data,  values = [\"depth_estimate\"], columns=['actual_depth'], aggfunc=np.mean)\n",
    "    avg.reset_index()\n",
    "    avg_renamed = avg.rename(index={'depth_estimate': 'Average Estimated Depth'})\n",
    "        \n",
    "    std = pd.pivot_table(data, values = [\"depth_estimate\"], columns = [\"actual_depth\"], aggfunc = np.std)\n",
    "    #note - std is normalized byN-1 by default (ddof parameter = 1 by default)\n",
    "    std.reset_index()\n",
    "    std_renamed = std.rename(index={'depth_estimate': 'Standard Deviation'})\n",
    "        \n",
    "    sem = pd.pivot_table(data, values = [\"depth_estimate\"], columns = [\"actual_depth\"], aggfunc = 'sem')\n",
    "    sem.reset_index()\n",
    "    sem_renamed = sem.rename(index={'depth_estimate': 'Standard Error'})\n",
    "        \n",
    "    frames = [avg_renamed, std_renamed, sem_renamed] #list of pivot tables for a given duration\n",
    "    result = pd.concat(frames) #merge the pivot tables for a given duration \n",
    "    result = result.T #transpose \n",
    "        \n",
    "    #Label the data by duration \n",
    "    result[\"Duration\"] = duration\n",
    "    \n",
    "    return result\n",
    "\n",
    "def subject_getxy(data):\n",
    "    \"\"\"\n",
    "    Extracts the data from the dataframes to a list format for plotting. \n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    ste = []\n",
    "    for idx, row in data.iterrows():\n",
    "        x.append(idx) #idx is the actual depth value \n",
    "            \n",
    "        estim_avg = row[\"Average Estimated Depth\"]\n",
    "        y.append(estim_avg)\n",
    "            \n",
    "        standard_error = row[\"Standard Error\"]\n",
    "        ste.append(standard_error)\n",
    "   \n",
    "    return x, y, ste \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnovaRM_subjectData(df, durations):\n",
    "    \"\"\"\n",
    "    Analyze data by each subject \n",
    "    Returns list of data by subject\n",
    "    \"\"\"\n",
    "    \n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    subj_slopes = {'subjID': [], 'duration': [], 'slope': [], 'age': []}\n",
    "    subj_intercepts = {'subjID': [], 'duration': [], 'intercept' : [], 'age': []}\n",
    "\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj] \n",
    "        for duration in durations:\n",
    "            duration_subj_df = subj_df\n",
    "            duration_subj_pivot = subject_pivotTable(duration_subj_df, duration)\n",
    "            duration_subj_data = subject_getxy(duration_subj_pivot)\n",
    "\n",
    "            x = np.array(duration_subj_data[0])\n",
    "            y = np.array(duration_subj_data[1])\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "            subj_slopes['subjID'].append(subj)\n",
    "            subj_slopes['duration'].append(duration)\n",
    "            subj_slopes['slope'].append(slope)\n",
    "            subj_slopes['age'].append(duration_subj_df.age.unique()[0])\n",
    "            \n",
    "            subj_intercepts['subjID'].append(subj)\n",
    "            subj_intercepts['duration'].append(duration)            \n",
    "            subj_intercepts['intercept'].append(intercept)\n",
    "            subj_intercepts['age'].append(duration_subj_df.age.unique()[0])\n",
    "                \n",
    "    slope_df = pd.DataFrame(data=subj_slopes)\n",
    "    intercept_df = pd.DataFrame(data=subj_intercepts)\n",
    "        \n",
    "    return slope_df, intercept_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_final_data_125 = zscored_data.loc[zscored_data[\"duration\"]  == 125]\n",
    "zs_final_data_250 = zscored_data.loc[zscored_data[\"duration\"]  == 250]\n",
    "zs_final_data_1000 = zscored_data.loc[zscored_data[\"duration\"]  == 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_125, df_intercept_125 = AnovaRM_subjectData(zs_final_data_125, [125])\n",
    "slopes_250, df_intercept_250 = AnovaRM_subjectData(zs_final_data_250, [250])\n",
    "slopes_1000, df_intercept_1000 = AnovaRM_subjectData(zs_final_data_1000, [1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Participant Average Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjIDs = zscored_data.subjID.unique()\n",
    "avgs = []\n",
    "for subj in all_subjIDs:\n",
    "    subj_df = zscored_data.loc[zscored_data['subjID'] == subj]\n",
    "    subj_avg = np.array(subj_df['depth_estimate']).mean()\n",
    "    avgs.append(subj_avg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [8,6])\n",
    "plt.title(\"Distribution of Participant Average Depth Estimations\", fontsize = 15)\n",
    "plt.ylabel(\"count\", fontsize = 12)\n",
    "plt.xlabel(\"Average Depth Estimate (meters)\", fontsize = 12)\n",
    "_, bins, _ = plt.hist(avgs, 50, density=1, alpha=0.6, color = 'black')\n",
    "mu, sigma = scipy.stats.norm.fit(avgs)\n",
    "best_fit_line = scipy.stats.norm.pdf(bins, mu, sigma)\n",
    "# plt.plot(bins, best_fit_line, color = 'orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split by duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_125ms = zscored_data[zscored_data['duration'] == 125]\n",
    "data_250ms = zscored_data[zscored_data['duration'] == 250]\n",
    "data_1000ms = zscored_data[zscored_data['duration'] == 1000]\n",
    "\n",
    "duration_data = [data_125ms, data_250ms, data_1000ms]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest = ''\n",
    "durations = ['125', '250', '1000']\n",
    "\n",
    "for i in range(len(duration_data)):  \n",
    "    duration_data[i].to_csv(dest + 'z_scored' + durations[i] + '_data.csv' , index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Target Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_pivotTable(data):\n",
    "    \"\"\"\n",
    "    Generate pivot tables from data after cleaning and outlier removal \n",
    "    Organizes data such that for each individual target (stimulus) the following is calulated:\n",
    "    - average depth estimation\n",
    "    - standard deviation\n",
    "    - standard error \n",
    "    \"\"\"\n",
    "    \n",
    "    avg_tables = []\n",
    "    std_tables = []\n",
    "    result_tables = []\n",
    "    ###### CREATE DF WITH DATA STATISTICS AFTER OUTLIER REMOVAL ######\n",
    "    cond = 0\n",
    "    for duration in data: #generate pivot tables for data statistics (avg, std, sem)\n",
    "        actual = pd.pivot_table(duration,  values = [\"actual_depth\"], columns=['stimulus'], aggfunc=np.mean)\n",
    "        actual.reset_index()\n",
    "        actual_renamed = actual.rename(index={'actual_depth': 'Actual Depth'})\n",
    "        \n",
    "        avg = pd.pivot_table(duration,  values = [\"depth_estimate\"], columns=['stimulus'], aggfunc=np.mean)\n",
    "        avg.reset_index()\n",
    "        avg_renamed = avg.rename(index={'depth_estimate': 'Average Estimated Depth'})\n",
    "        \n",
    "        RT = pd.pivot_table(duration,  values = [\"trial_RT\"], columns=['stimulus'], aggfunc=np.mean)\n",
    "        RT.reset_index()\n",
    "        RT_renamed = RT.rename(index={'trial_RT': 'Average Trial RT'})\n",
    "        \n",
    "        std = pd.pivot_table(duration, values = [\"depth_estimate\"], columns = [\"stimulus\"], aggfunc = np.std)\n",
    "        #note - std is normalized byN-1 by default (ddof parameter = 1 by default)\n",
    "        std.reset_index()\n",
    "        std_renamed = std.rename(index={'depth_estimate': 'Standard Deviation'})\n",
    "        \n",
    "        sem = pd.pivot_table(duration, values = [\"depth_estimate\"], columns = [\"stimulus\"], aggfunc = 'sem')\n",
    "        sem.reset_index()\n",
    "        sem_renamed = sem.rename(index={'depth_estimate': 'Standard Error'})\n",
    "        \n",
    "        frames = [avg_renamed, std_renamed, sem_renamed, actual_renamed, RT_renamed] #list of pivot tables for a given duration\n",
    "        result = pd.concat(frames) #merge the pivot tables for a given duration \n",
    "        result = result.T #transpose \n",
    "        result = result.sort_values(by=['Actual Depth'])\n",
    "\n",
    "        #Label the data by duration based on condition counter (cond)\n",
    "        if cond == 0:\n",
    "            result[\"Duration\"] = 125\n",
    "        if cond == 1:\n",
    "            result[\"Duration\"] = 250\n",
    "        if cond == 2:\n",
    "            result[\"Duration\"] = 1000\n",
    "        \n",
    "        avg_tables.append(avg_renamed) #created for reference (not used in code)\n",
    "        std_tables.append(std_renamed) #created for reference (not used in code)\n",
    "        result_tables.append(result) #list of results for all durations \n",
    "        cond += 1 \n",
    "        \n",
    "    \n",
    "    return result_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_trial_pivot = trial_pivotTable(duration_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_trial_pivot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_getxy(data):\n",
    "    \"\"\"\n",
    "    Extracts the data from the dataframes to a list format for plotting. \n",
    "    Args:\n",
    "        df = [125, 250, 1000]\n",
    "        These data frames are POST all outlier cleaning. \n",
    "        \n",
    "    Returns:\n",
    "        actualdepths = [x_125, x_250, x_1000]\n",
    "        xs = [list of individual targets]\n",
    "        ys = [y_125, y_250, y_1000]\n",
    "        stes = [ste_125, ste_250, ste_1000]\n",
    "        \n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    stes = []\n",
    "    stds = []\n",
    "    actualdepths = []\n",
    "    trial_RTs = []\n",
    "    for table in data:\n",
    "        x = []\n",
    "        y = []\n",
    "        ste = []\n",
    "        std = []\n",
    "        depths = []\n",
    "        RT = []\n",
    "        for idx, row in table.iterrows():\n",
    "            \n",
    "            x.append(idx) #idx is the target (stimulus path)\n",
    "            \n",
    "            estim_avg = row[\"Average Estimated Depth\"]\n",
    "            y.append(estim_avg)\n",
    "            \n",
    "            standard_error = row[\"Standard Error\"]\n",
    "            ste.append(standard_error)\n",
    "            \n",
    "            depth = row[\"Actual Depth\"]\n",
    "            depths.append(depth)\n",
    "            \n",
    "            standard_deviation = row[\"Standard Deviation\"]\n",
    "            std.append(standard_deviation)       \n",
    "            \n",
    "            reactionTime = row[\"Average Trial RT\"]\n",
    "            RT.append(reactionTime)  \n",
    "            \n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        stes.append(ste)\n",
    "        actualdepths.append(depths)\n",
    "        stds.append(std)\n",
    "        trial_RTs.append(RT)\n",
    "\n",
    "    return xs, ys, stes, actualdepths, stds, trial_RTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_raw_final = trial_getxy(raw_trial_pivot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_raw_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_plot_data = trial_raw_final\n",
    "\n",
    "## Execute this cell to prep for plotting\n",
    "\n",
    "final_x_125 = trial_plot_data[0][0]\n",
    "final_y_125 = trial_plot_data[1][0]\n",
    "ste_125 = trial_plot_data[2][0]\n",
    "stim_125 = trial_plot_data[3][0]\n",
    "std_125 = trial_plot_data[4][0]\n",
    "RT_125 = trial_plot_data[5][0]\n",
    "\n",
    "final_x_250 = trial_plot_data[0][1]\n",
    "final_y_250 = trial_plot_data[1][1]\n",
    "ste_250 = trial_plot_data[2][1]\n",
    "stim_250 = trial_plot_data[3][1]\n",
    "std_250 = trial_plot_data[4][1]\n",
    "RT_250 = trial_plot_data[5][1]\n",
    "\n",
    "final_x_1000 = trial_plot_data[0][2]\n",
    "final_y_1000 = trial_plot_data[1][2]\n",
    "ste_1000 = trial_plot_data[2][2]\n",
    "stim_1000 = trial_plot_data[3][2]\n",
    "std_1000 = trial_plot_data[4][2]\n",
    "RT_1000 = trial_plot_data[5][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7567623905880488, 3.116956320610722e-30)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.pearsonr(final_y_125, final_y_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7227090510628403, 1.739275130706995e-26)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.pearsonr(final_y_125, final_y_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9438974933698375, 5.391292496703656e-76)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.pearsonr(final_y_250, final_y_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.figure(figsize = [8,8])\n",
    "#run regression\n",
    "X = np.array(stim_125).reshape(-1,1)\n",
    "y = final_y_125\n",
    "ste = ste_125\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "#Generated Predictions\n",
    "y_predicted = reg.predict(X)\n",
    "#Plot Our Actual and Predicted Values\n",
    "plt.plot(X, y, 'o', color='black', alpha = 0.5);\n",
    "plt.plot(X,y_predicted,color='darkgreen', label = 'm = ' + str(round(reg.coef_[0], 3))\n",
    "         + '     r-squared = ' + str(round(float(reg.score(X, y)), 3)))\n",
    "plt.title(\"125 ms\", fontsize = 20)\n",
    "plt.xlabel(\"Actual Depth (m)\", fontsize = 15)\n",
    "plt.ylabel(\"z-scored Estimated Depth (m)\", fontsize = 15)\n",
    "# plt.plot(X, X, label = \"Perfect Accuracy\", color = 'black',linestyle='--')  # solid\n",
    "plt.errorbar(X, y, yerr=ste, elinewidth = 1, ecolor = \"gray\", fmt = 'or', mfc = \"darkgreen\", mec = \"darkgreen\", capsize = 3)\n",
    "\n",
    "legend = plt.legend(loc = 0, fontsize = 13, borderpad = 0.6, labelspacing = 1)\n",
    "legend.get_frame().set_facecolor('lightgray')\n",
    "\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(reg.coef_))\n",
    "print(\"b: {0}\".format(reg.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", reg.score(X, y))\n",
    "\n",
    "round(float(reg.score(X, y)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.figure(figsize = [8,8])\n",
    "#run regression\n",
    "X = np.array(final_y_250).reshape(-1,1)\n",
    "y = RT_250\n",
    "ste = ste_250\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "#Generated Predictions\n",
    "y_predicted = reg.predict(X)\n",
    "#Plot Our Actual and Predicted Values\n",
    "plt.plot(X, y, 'o', color='black', alpha = 0.5);\n",
    "plt.plot(X,y_predicted,color='chocolate', label = 'm = ' + str(round(reg.coef_[0], 3))\n",
    "         + '     r-squared = ' + str(round(float(reg.score(X, y)), 3)))\n",
    "plt.title(\"250 ms\", fontsize = 20)\n",
    "plt.xlabel(\"z-scored Estimated Depth\", fontsize = 15)\n",
    "plt.ylabel(\"z-scored RT\", fontsize = 15)\n",
    "plt.errorbar(X, y, yerr=ste, elinewidth = 1, ecolor = \"gray\", fmt = 'or', mfc = \"chocolate\", mec = \"chocolate\", capsize = 3)\n",
    "\n",
    "legend = plt.legend(loc = 0, fontsize = 13, borderpad = 0.6, labelspacing = 1)\n",
    "legend.get_frame().set_facecolor('lightgray')\n",
    "\n",
    "\n",
    "#get coefficients and y intercept\n",
    "print(\"m: {0}\".format(reg.coef_))\n",
    "print(\"b: {0}\".format(reg.intercept_))\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "print(\"R-squared: \", reg.score(X, y))\n",
    "\n",
    "round(float(reg.score(X, y)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_destpath = ''\n",
    "\n",
    "with open(n_destpath + 'X_250.npy', 'wb') as f:\n",
    "    np.save(f, X_250)\n",
    "with open(n_destpath + 'final_y_250.npy', 'wb') as f:\n",
    "    np.save(f, final_y_250)\n",
    "    \n",
    "with open(n_destpath + 'X_1000.npy', 'wb') as f:\n",
    "    np.save(f, X_1000)\n",
    "with open(n_destpath + 'final_y_1000.npy', 'wb') as f:\n",
    "    np.save(f, final_y_1000)\n",
    "\n",
    "with open(n_destpath + 'std_250.npy', 'wb') as f:\n",
    "    np.save(f, std_250)\n",
    "    \n",
    "with open(n_destpath + 'std_1000.npy', 'wb') as f:\n",
    "    np.save(f, std_1000)\n",
    "    \n",
    "with open(n_destpath + 'ste_250.npy', 'wb') as f:\n",
    "    np.save(f, ste_250)\n",
    "    \n",
    "with open(n_destpath + 'ste_1000.npy', 'wb') as f:\n",
    "    np.save(f, ste_1000)\n",
    "\n",
    "with open(n_destpath + 'final_stim_250.npy', 'wb') as f:\n",
    "    np.save(f, final_x_250)\n",
    "\n",
    "with open(n_destpath + 'final_stim_1000.npy', 'wb') as f:\n",
    "    np.save(f, final_x_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Way Repeated Measures ANOVA\n",
    "\n",
    "https://statistics.laerd.com/statistical-guides/repeated-measures-anova-statistical-guide.php\n",
    "\n",
    "IV conditions: Duration \n",
    "\n",
    "IV: Actual Depth\n",
    "\n",
    "DV: Depth Estimate \n",
    "\n",
    "H0: µ1 = µ2 = µ3 = … = µk where µ = population mean and k = number of related groups. \n",
    "\n",
    "HA: at least two means are significantly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "\n",
    "def subject_pivotTable(data, duration):\n",
    "    \"\"\"\n",
    "    Generate pivot tables from data after cleaning and outlier removal \n",
    "    Organizes data such that for each rounded actual depth the following is calulated:\n",
    "    - average depth estimation\n",
    "    - standard deviation\n",
    "    - standard error \n",
    "    \"\"\"\n",
    "\n",
    "    ###### CREATE DF WITH DATA STATISTICS AFTER OUTLIER REMOVAL ######\n",
    "    avg = pd.pivot_table(data,  values = [\"depth_estimate\"], columns=['actual_depth'], aggfunc=np.mean)\n",
    "    avg.reset_index()\n",
    "    avg_renamed = avg.rename(index={'depth_estimate': 'Average Estimated Depth'})\n",
    "        \n",
    "    std = pd.pivot_table(data, values = [\"depth_estimate\"], columns = [\"actual_depth\"], aggfunc = np.std)\n",
    "    #note - std is normalized byN-1 by default (ddof parameter = 1 by default)\n",
    "    std.reset_index()\n",
    "    std_renamed = std.rename(index={'depth_estimate': 'Standard Deviation'})\n",
    "        \n",
    "    sem = pd.pivot_table(data, values = [\"depth_estimate\"], columns = [\"actual_depth\"], aggfunc = 'sem')\n",
    "    sem.reset_index()\n",
    "    sem_renamed = sem.rename(index={'depth_estimate': 'Standard Error'})\n",
    "        \n",
    "    frames = [avg_renamed, std_renamed, sem_renamed] #list of pivot tables for a given duration\n",
    "    result = pd.concat(frames) #merge the pivot tables for a given duration \n",
    "    result = result.T #transpose \n",
    "        \n",
    "    #Label the data by duration \n",
    "    result[\"Duration\"] = duration\n",
    "    \n",
    "    return result\n",
    "\n",
    "def subject_getxy(data):\n",
    "    \"\"\"\n",
    "    Extracts the data from the dataframes to a list format for plotting. \n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    ste = []\n",
    "    for idx, row in data.iterrows():\n",
    "        x.append(idx) #idx is the actual depth value \n",
    "            \n",
    "        estim_avg = row[\"Average Estimated Depth\"]\n",
    "        y.append(estim_avg)\n",
    "            \n",
    "        standard_error = row[\"Standard Error\"]\n",
    "        ste.append(standard_error)\n",
    "   \n",
    "    return x, y, ste \n",
    "\n",
    "def subjectData(df, durations):\n",
    "    \"\"\"\n",
    "    Analyze data by each subject \n",
    "    Returns list of data by subject\n",
    "    \"\"\"\n",
    "    \n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    subj_slopes_intercepts = {'subjID': [], 'duration': [], 'block': [], 'slope': [], 'intercept' : []}\n",
    "\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj] \n",
    "        block1 = subj_df.loc[subj_df['trial'] < num_trials/4]\n",
    "        temp_block2 = subj_df.loc[subj_df['trial'] <= (num_trials/4)*2]\n",
    "        block2 = temp_block2.loc[temp_block2['trial'] >= num_trials/4]\n",
    "        temp_block3 = subj_df.loc[subj_df['trial'] <= (num_trials/4)*3]\n",
    "        block3 = temp_block3.loc[temp_block3['trial'] > (num_trials/4)*2]\n",
    "        block4 = subj_df.loc[subj_df['trial'] >= (num_trials/4)*3]\n",
    "        blocks = [block1, block2, block3, block4]\n",
    "        \n",
    "        num_block = 1\n",
    "        for block in blocks:\n",
    "            for duration in durations:\n",
    "                block_duration_subj_df = block.loc[block[\"duration\"]  == duration]\n",
    "                block_duration_subj_pivot = subject_pivotTable(block_duration_subj_df, duration)\n",
    "                block_duration_subj_data = subject_getxy(block_duration_subj_pivot)\n",
    "\n",
    "                x = np.array(block_duration_subj_data[0])\n",
    "                y = np.array(block_duration_subj_data[1])\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "                subj_slopes_intercepts['subjID'].append(subj)\n",
    "                subj_slopes_intercepts['duration'].append(duration)\n",
    "                subj_slopes_intercepts['block'].append(num_block)\n",
    "                subj_slopes_intercepts['slope'].append(slope)\n",
    "                subj_slopes_intercepts['intercept'].append(intercept)\n",
    "                \n",
    "            num_block += 1\n",
    "\n",
    "    block_duration_df = pd.DataFrame(data=subj_slopes_intercepts)\n",
    "        \n",
    "    return block_duration_df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_duration_block_slopes_intercepts = subjectData(final_data, [125, 250, 1000], num_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_duration_block_slopes_intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnovaRM_subjectData(df, durations):\n",
    "    \"\"\"\n",
    "    Analyze data by each subject \n",
    "    Returns list of data by subject\n",
    "    \"\"\"\n",
    "    \n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    subj_slopes = {'subjID': [], 'duration': [], 'slope': []}\n",
    "    subj_intercepts = {'subjID': [], 'duration': [], 'intercept' : []}\n",
    "\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj] \n",
    "\n",
    "        for duration in durations:\n",
    "            duration_subj_df = subj_df.loc[subj_df[\"duration\"]  == duration]\n",
    "            duration_subj_pivot = subject_pivotTable(duration_subj_df, duration)\n",
    "            duration_subj_data = subject_getxy(duration_subj_pivot)\n",
    "\n",
    "            x = np.array(duration_subj_data[0])\n",
    "            y = np.array(duration_subj_data[1])\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "            subj_slopes['subjID'].append(subj)\n",
    "            subj_slopes['duration'].append(duration)\n",
    "            subj_slopes['slope'].append(slope)\n",
    "            \n",
    "            subj_intercepts['subjID'].append(subj)\n",
    "            subj_intercepts['duration'].append(duration)            \n",
    "            subj_intercepts['intercept'].append(intercept)\n",
    "                \n",
    "    slope_df = pd.DataFrame(data=subj_slopes)\n",
    "    intercept_df = pd.DataFrame(data=subj_intercepts)\n",
    "        \n",
    "    return slope_df, intercept_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_slope, df_intercept = AnovaRM_subjectData(final_data, [125, 250, 1000])\n",
    "\n",
    "df_slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_125 = df_slope.loc[df_slope['duration'] == 125]\n",
    "slopes_250 = df_slope.loc[df_slope['duration'] == 250]\n",
    "slopes_1000 = df_slope.loc[df_slope['duration'] == 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANCOVA_subjectData(df, durations):\n",
    "    \"\"\"\n",
    "    Analyze data by each subject \n",
    "    Returns list of data by subject\n",
    "    \"\"\"\n",
    "    \n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    subj_slopes = {'subjID': [], 'duration': [], 'slope': [], 'age': []}\n",
    "    subj_intercepts = {'subjID': [], 'duration': [], 'intercept' : [], 'age': []}\n",
    "\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj] \n",
    "\n",
    "        for duration in durations:\n",
    "            duration_subj_df = subj_df.loc[subj_df[\"duration\"]  == duration]\n",
    "            duration_subj_pivot = subject_pivotTable(duration_subj_df, duration)\n",
    "            duration_subj_data = subject_getxy(duration_subj_pivot)\n",
    "            \n",
    "            x = np.array(duration_subj_data[0])\n",
    "            y = np.array(duration_subj_data[1])\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "            subj_slopes['subjID'].append(subj)\n",
    "            subj_slopes['duration'].append(duration)\n",
    "            subj_slopes['slope'].append(slope)\n",
    "            subj_slopes['age'].append(duration_subj_df.age.unique()[0])\n",
    "            \n",
    "            subj_intercepts['subjID'].append(subj)\n",
    "            subj_intercepts['duration'].append(duration)            \n",
    "            subj_intercepts['intercept'].append(intercept)\n",
    "            subj_intercepts['age'].append(duration_subj_df.age.unique()[0])\n",
    "\n",
    "\n",
    "                \n",
    "    slope_df = pd.DataFrame(data=subj_slopes)\n",
    "    intercept_df = pd.DataFrame(data=subj_intercepts)\n",
    "        \n",
    "    return slope_df, intercept_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age_slope, df_age_intercept = ANCOVA_subjectData(final_data, [125, 250, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age_slope_125 = df_age_slope.loc[df_age_slope['duration'] == 125] \n",
    "df_age_slope_125_mean = df_age_slope_125.groupby('age').mean()\n",
    "\n",
    "df_age_slope_250 = df_age_slope.loc[df_age_slope['duration'] == 250] \n",
    "df_age_slope_250_mean = df_age_slope_250.groupby('age').mean()\n",
    "\n",
    "df_age_slope_1000 = df_age_slope.loc[df_age_slope['duration'] == 1000] \n",
    "df_age_slope_1000_mean = df_age_slope_1000.groupby('age').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = ([10, 8]))\n",
    "plt.xlabel(\"Age\", fontsize = 17)\n",
    "plt.ylabel(\"Subject Slope\", fontsize = 17)\n",
    "\n",
    "ages_125 = df_age_slope_125_mean.index\n",
    "slope_125, intercept_125, r_value_125, p_value_125, std_err_125 = stats.linregress(ages_125, df_age_slope_125_mean['slope'])\n",
    "plt.plot(ages_125, df_age_slope_125_mean['slope'], label = '125 ms: m = ' + str(round(slope_125, 4)), color = \"blue\")\n",
    "plt.plot(df_age_slope_125_mean['slope'], 'o', color = \"blue\")\n",
    "print(r_value_125 **2)\n",
    "\n",
    "\n",
    "ages_250 = df_age_slope_250_mean.index\n",
    "slope_250, intercept_250, r_value_250, p_value_250, std_err_250 = stats.linregress(ages_250, df_age_slope_250_mean['slope'])\n",
    "plt.plot(ages_250, df_age_slope_250_mean['slope'], label = '250 ms: m = ' + str(round(slope_250, 4)), color = \"blue\")\n",
    "# plt.plot(ages, intercept_250 + slope_250*ages, 'r', color = \"blue\")\n",
    "print(r_value_250 **2)\n",
    "\n",
    "ages_1000 = df_age_slope_1000_mean.index\n",
    "slope_1000, intercept_1000, r_value_1000, p_value_1000, std_err_1000 = stats.linregress(ages_1000, df_age_slope_1000_mean['slope'])\n",
    "plt.plot(ages_1000, df_age_slope_1000_mean['slope'], label = '1000 ms: m = ' + str(round(slope_1000, 4)), color = \"purple\")\n",
    "# plt.plot(ages, intercept_1000 + slope_1000*ages, 'r',  color = \"purple\")\n",
    "print(r_value_1000 **2)\n",
    "\n",
    "plt.axhline(y=1, color = 'black',linestyle='--', label = \"Perfect Accuracy Slope\")\n",
    "\n",
    "\n",
    "legend = plt.legend(loc = 0, fontsize = 12, borderpad = 0.8, labelspacing = 1)\n",
    "legend.get_frame().set_facecolor('lightgray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pingouin-stats.org/generated/pingouin.ancova.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pingouin import ancova, read_dataset\n",
    "\n",
    "print(\"DV: Slope\")\n",
    "ancova(data=df_age_slope, dv='slope', covar='age', between='duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DV: Intercept\")\n",
    "ancova(data=df_age_intercept, dv='intercept', covar='age', between='duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DV: Normalized Slope\")\n",
    "ancova(data=norm_df_age_slope, dv='slope', covar='age', between='duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DV: Normalized Intercept\")\n",
    "ancova(data=norm_df_age_intercept, dv='intercept', covar='age', between='duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [16, 9])\n",
    "plt.xticks(np.arange(20, 80, 5), fontsize = 13)\n",
    "plt.yticks(np.arange(0, 1.8, 0.1))\n",
    "plt.xlabel(\"Age\", fontsize = 17)\n",
    "plt.ylabel(\"Subject Slope\", fontsize = 17)\n",
    "\n",
    "ages = df_age_slope_250_mean.index\n",
    "\n",
    "slope_125, intercept_125, r_value_125, p_value_125, std_err_125 = stats.linregress(ages, df_age_slope_125_mean['slope'])\n",
    "plt.plot(ages, df_age_slope_125_mean['slope'], label = '125 ms: m = ' + str(round(slope_125, 4)), color = \"orange\")\n",
    "# plt.plot(ages, intercept_125 + slope_125*ages, 'r',  color = \"orange\")\n",
    "print(r_value_125 **2)\n",
    "\n",
    "slope_250, intercept_250, r_value_250, p_value_250, std_err_250 = stats.linregress(ages, df_age_slope_250_mean['slope'])\n",
    "plt.plot(ages, df_age_slope_250_mean['slope'], label = '250 ms: m = ' + str(round(slope_250, 4)), color = \"blue\")\n",
    "# plt.plot(ages, intercept_250 + slope_250*ages, 'r', color = \"blue\")\n",
    "print(r_value_250 **2)\n",
    "\n",
    "slope_1000, intercept_1000, r_value_1000, p_value_1000, std_err_1000 = stats.linregress(ages, df_age_slope_1000_mean['slope'])\n",
    "plt.plot(ages, df_age_slope_1000_mean['slope'], label = '1000 ms: m = ' + str(round(slope_1000, 4)), color = \"purple\")\n",
    "# plt.plot(ages, intercept_1000 + slope_1000*ages, 'r',  color = \"purple\")\n",
    "print(r_value_1000 **2)\n",
    "\n",
    "plt.axhline(y=1/average_depth_estimate, color = 'black',linestyle='--', label = \"Perfect Accuracy Slope\")\n",
    "\n",
    "plt.title(\"Normalized Data\", fontsize = 23)\n",
    "legend = plt.legend(loc = 0, fontsize = 12, borderpad = 0.8, labelspacing = 1)\n",
    "legend.get_frame().set_facecolor('lightgray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
