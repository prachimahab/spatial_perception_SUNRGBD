{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export VE Data as DataFrame for LME Cue Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals, clutter, viewing duration, stimulus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/verbal_judgement_analysis/data/finalVEMatched/z_scored/'\n",
    "\n",
    "# dpath = '/Users/pmahableshwarkar/Documents/Depth_Project/verbal_judgement_analysis/data/finalVEMatched/z_scored/'\n",
    "\n",
    "# stim \n",
    "with open(dpath + 'z_final_stim_125.npy' , 'rb') as f:\n",
    "    stim_125 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "with open(dpath + 'z_final_stim_250.npy' , 'rb') as f:\n",
    "    stim_250 = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(dpath + 'z_final_stim_1000.npy' , 'rb') as f:\n",
    "    stim_1000 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "# residuals\n",
    "with open(dpath + 'residuals/'+ 'z_residuals_125.npy' , 'rb') as f:\n",
    "    residuals_125 = np.load(f, allow_pickle=True)\n",
    "with open(dpath + 'residuals/'+ 'z_residuals_250.npy' , 'rb') as f:\n",
    "    residuals_250 = np.load(f, allow_pickle=True) \n",
    "with open(dpath + 'residuals/'+ 'z_residuals_1000.npy' , 'rb') as f:\n",
    "    residuals_1000= np.load(f, allow_pickle=True)\n",
    "    \n",
    "# difference between z-scored estimate and z-scored actual depth\n",
    "with open(dpath + '/diff_125.npy' , 'rb') as f:\n",
    "    diff_125 = np.load(f, allow_pickle=True)\n",
    "with open(dpath + '/diff_250.npy' , 'rb') as f:\n",
    "    diff_250 = np.load(f, allow_pickle=True) \n",
    "with open(dpath + '/diff_1000.npy' , 'rb') as f:\n",
    "    diff_1000= np.load(f, allow_pickle=True)\n",
    "    \n",
    "# difference between raw estimate and raw actual depth\n",
    "# with open('/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/verbal_judgement_analysis/data/finalVEMatched/raw' + '/raw_diff_125.npy' , 'rb') as f:\n",
    "#     raw_diff_125 = np.load(f, allow_pickle=True)\n",
    "# with open('/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/verbal_judgement_analysis/data/finalVEMatched/raw' + '/raw_diff_250.npy' , 'rb') as f:\n",
    "#     raw_diff_250 = np.load(f, allow_pickle=True) \n",
    "# with open('/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/verbal_judgement_analysis/data/finalVEMatched/raw' + '/raw_diff_1000.npy' , 'rb') as f:\n",
    "#     raw_diff_1000= np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/verbal_judgement_analysis/cue_analyses/cleaned_globalClutter.json')\n",
    "# f = open('/Users/pmahableshwarkar/Documents/Depth_Project/verbal_judgement_analysis/cue_analyses/cleaned_globalClutter.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "cleaned_globalClutter = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_globalClutter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_125 = [elem.split('/')[1][:-2] for elem in stim_125]\n",
    "f_250 = [elem.split('/')[1][:-2] for elem in stim_250]\n",
    "f_1000 = [elem.split('/')[1][:-2] for elem in stim_1000]\n",
    "\n",
    "len(set(f_125))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = {}\n",
    "for f in f_125:\n",
    "    if f not in cleaned_globalClutter.keys():\n",
    "        missing[f] = None\n",
    "\n",
    "# len(missing), missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(missing_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Image\n",
    "\n",
    "kinect192 = '/Users/prachimahableshwarkar/Documents/GW/SUNRGBD/updated/kinect2data_192/'\n",
    "\n",
    "missing_img_paths = []\n",
    "for f in k:\n",
    "    p = kinect192 + f + '/image' \n",
    "    for file in os.listdir(p):\n",
    "        full_p = p + '/' + file\n",
    "        missing_img_paths.append(full_p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 114, 114, 114, 114)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalClutter = []\n",
    "\n",
    "trunc_residuals_125 = []\n",
    "trunc_residuals_250 = []\n",
    "trunc_residuals_1000 = []\n",
    "\n",
    "stimulus = []\n",
    "\n",
    "for stim in cleaned_globalClutter:\n",
    "\n",
    "    try:\n",
    "        index_125 = f_125.index(stim)\n",
    "        index_250 = f_250.index(stim)\n",
    "        index_1000 = f_1000.index(stim)\n",
    "\n",
    "        trunc_residuals_125.append(residuals_125[index_125])\n",
    "        trunc_residuals_250.append(residuals_250[index_250])\n",
    "        trunc_residuals_1000.append(residuals_1000[index_1000])\n",
    "\n",
    "        globalClutter.append(cleaned_globalClutter[stim])\n",
    "\n",
    "        stimulus.append(stim)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "len(trunc_residuals_125), len(trunc_residuals_250), len(trunc_residuals_1000), len(globalClutter), len(stimulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([125] * len(trunc_residuals_125))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.concatenate((np.abs(np.array(trunc_residuals_125)), np.abs(np.array(trunc_residuals_250)), np.abs(np.array(trunc_residuals_1000))), axis=0),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>residuals</th>\n",
       "      <th>abs_residuals</th>\n",
       "      <th>duration</th>\n",
       "      <th>clutter</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.099975</td>\n",
       "      <td>0.099975</td>\n",
       "      <td>125</td>\n",
       "      <td>3</td>\n",
       "      <td>000866_2014-06-09_20-45-42_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.080464</td>\n",
       "      <td>0.080464</td>\n",
       "      <td>125</td>\n",
       "      <td>19</td>\n",
       "      <td>003070_2014-06-15_14-58-27_094959634447_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.301428</td>\n",
       "      <td>0.301428</td>\n",
       "      <td>125</td>\n",
       "      <td>11</td>\n",
       "      <td>001092_2014-06-15_17-34-58_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.075657</td>\n",
       "      <td>0.075657</td>\n",
       "      <td>125</td>\n",
       "      <td>14</td>\n",
       "      <td>002272_2014-06-28_18-53-56_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.164808</td>\n",
       "      <td>0.164808</td>\n",
       "      <td>125</td>\n",
       "      <td>23</td>\n",
       "      <td>000642_2014-06-08_16-59-25_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.084265</td>\n",
       "      <td>0.084265</td>\n",
       "      <td>1000</td>\n",
       "      <td>12</td>\n",
       "      <td>001094_2014-06-15_17-36-00_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0.127303</td>\n",
       "      <td>0.127303</td>\n",
       "      <td>1000</td>\n",
       "      <td>24</td>\n",
       "      <td>001808_2014-06-26_20-50-58_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.341774</td>\n",
       "      <td>0.341774</td>\n",
       "      <td>1000</td>\n",
       "      <td>14</td>\n",
       "      <td>000758_2014-06-08_22-05-08_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0.322175</td>\n",
       "      <td>0.322175</td>\n",
       "      <td>1000</td>\n",
       "      <td>7</td>\n",
       "      <td>000878_2014-06-08_22-52-45_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>-0.224550</td>\n",
       "      <td>0.224550</td>\n",
       "      <td>1000</td>\n",
       "      <td>12</td>\n",
       "      <td>001167_2014-06-17_15-38-07_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     residuals  abs_residuals  duration  clutter  \\\n",
       "0    -0.099975       0.099975       125        3   \n",
       "1     0.080464       0.080464       125       19   \n",
       "2    -0.301428       0.301428       125       11   \n",
       "3    -0.075657       0.075657       125       14   \n",
       "4    -0.164808       0.164808       125       23   \n",
       "..         ...            ...       ...      ...   \n",
       "337   0.084265       0.084265      1000       12   \n",
       "338   0.127303       0.127303      1000       24   \n",
       "339   0.341774       0.341774      1000       14   \n",
       "340   0.322175       0.322175      1000        7   \n",
       "341  -0.224550       0.224550      1000       12   \n",
       "\n",
       "                                              stimulus  \n",
       "0    000866_2014-06-09_20-45-42_260595134347_rgbf00...  \n",
       "1    003070_2014-06-15_14-58-27_094959634447_rgbf00...  \n",
       "2    001092_2014-06-15_17-34-58_260595134347_rgbf00...  \n",
       "3    002272_2014-06-28_18-53-56_260595134347_rgbf00...  \n",
       "4    000642_2014-06-08_16-59-25_260595134347_rgbf00...  \n",
       "..                                                 ...  \n",
       "337  001094_2014-06-15_17-36-00_260595134347_rgbf00...  \n",
       "338  001808_2014-06-26_20-50-58_260595134347_rgbf00...  \n",
       "339  000758_2014-06-08_22-05-08_260595134347_rgbf00...  \n",
       "340  000878_2014-06-08_22-52-45_260595134347_rgbf00...  \n",
       "341  001167_2014-06-17_15-38-07_260595134347_rgbf00...  \n",
       "\n",
       "[342 rows x 5 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize data of lists.\n",
    "data = {'residuals': trunc_residuals_125+trunc_residuals_250+trunc_residuals_1000,\n",
    "        'abs_residuals': np.concatenate((np.abs(np.array(trunc_residuals_125)), np.abs(np.array(trunc_residuals_250)), np.abs(np.array(trunc_residuals_1000))), axis=0),\n",
    "        'duration': [125] * len(trunc_residuals_125) + [250] * len(trunc_residuals_250) + [1000] * len(trunc_residuals_1000),\n",
    "        'clutter': globalClutter * 3,\n",
    "        'stimulus': stimulus * 3}\n",
    "  \n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stimulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clutter_data_for_lme.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Participant Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = '/Users/pmahableshwarkar/Documents/Depth_Project/verbal_judgement_analysis/data/finalVEMatched/z_scored/residuals/participant_residuals.csv'\n",
    "p = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/verbal_judgement_analysis/data/finalVEMatched/z_scored/residuals/participant_residuals.csv'\n",
    "\n",
    "p_data = pd.read_csv(p) \n",
    "p_stimuli = list(p_data.stimulus.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, '001808_2014-06-26_20-50-58_260595134347_rgbf000029-resize')"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_data['stimulus'].unique()), p_data['stimulus'].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incl_stimuli = stimulus\n",
    "\n",
    "# images that we do not have ground plane and/or clutter data for \n",
    "excluded_stimuli = [x for x in p_stimuli if x not in incl_stimuli]\n",
    "\n",
    "len(excluded_stimuli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all rows that have data for images that we do not have ground plane and/or clutter data for\n",
    "p_data = p_data[p_data.stimulus.isin(excluded_stimuli) == False]\n",
    "\n",
    "len(p_data.stimulus.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/create-new-column-based-on-other-columns-pandas-5586d87de73d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignClutter(row):  \n",
    "    stim = row['stimulus']\n",
    "    clutter = cleaned_globalClutter[stim]\n",
    "    \n",
    "    return clutter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clutter data to the df by referencing the GP value for each image\n",
    "p_data['clutter'] = p_data.apply(lambda row: assignClutter(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data['duration'] = p_data['duration'].div(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data.to_csv('clutter_participantData_for_lme.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = p_data.columns.tolist()\n",
    "cols = ['duration', 'subjID', 'stimulus', 's_residual', 'abs_s_residual', 'abs_error', 'error','clutter', 'actual_depth']\n",
    "\n",
    "p_data = p_data[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['duration', 'subjID', 'stimulus', 's_residual', 'abs_s_residual',\n",
       "       'abs_error', 'error', 'clutter', 'actual_depth'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data.columns[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data_grouped = p_data.groupby('stimulus')[p_data.columns[2:]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data_grouped.to_csv('clutter_participantData_grouped_for_lme.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
