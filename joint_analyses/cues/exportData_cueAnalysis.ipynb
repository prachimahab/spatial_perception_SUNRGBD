{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export VE Data as DataFrame for LME Cue Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals, clutter, complete ground plane, viewing duration, stimulus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dpath = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/verbal_judgement_analysis/data/finalVEMatched/z_scored/'\n",
    "\n",
    "dpath = '/Users/pmahableshwarkar/Documents/Depth_Project/verbal_judgement_analysis/data/finalVEMatched/z_scored/'\n",
    "\n",
    "# stim \n",
    "with open(dpath + 'z_final_stim_125.npy' , 'rb') as f:\n",
    "    stim_125 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "with open(dpath + 'z_final_stim_250.npy' , 'rb') as f:\n",
    "    stim_250 = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(dpath + 'z_final_stim_1000.npy' , 'rb') as f:\n",
    "    stim_1000 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "# residuals\n",
    "with open(dpath + 'residuals/'+ 'z_residuals_125.npy' , 'rb') as f:\n",
    "    residuals_125 = np.load(f, allow_pickle=True)\n",
    "with open(dpath + 'residuals/'+ 'z_residuals_250.npy' , 'rb') as f:\n",
    "    residuals_250 = np.load(f, allow_pickle=True) \n",
    "with open(dpath + 'residuals/'+ 'z_residuals_1000.npy' , 'rb') as f:\n",
    "    residuals_1000= np.load(f, allow_pickle=True)\n",
    "    \n",
    "# difference between z-scored estimate and z-scored actual depth\n",
    "with open(dpath + '/diff_125.npy' , 'rb') as f:\n",
    "    diff_125 = np.load(f, allow_pickle=True)\n",
    "with open(dpath + '/diff_250.npy' , 'rb') as f:\n",
    "    diff_250 = np.load(f, allow_pickle=True) \n",
    "with open(dpath + '/diff_1000.npy' , 'rb') as f:\n",
    "    diff_1000= np.load(f, allow_pickle=True)\n",
    "    \n",
    "# difference between raw estimate and raw actual depth\n",
    "# with open('/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/verbal_judgement_analysis/data/finalVEMatched/raw' + '/raw_diff_125.npy' , 'rb') as f:\n",
    "#     raw_diff_125 = np.load(f, allow_pickle=True)\n",
    "# with open('/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/verbal_judgement_analysis/data/finalVEMatched/raw' + '/raw_diff_250.npy' , 'rb') as f:\n",
    "#     raw_diff_250 = np.load(f, allow_pickle=True) \n",
    "# with open('/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/verbal_judgement_analysis/data/finalVEMatched/raw' + '/raw_diff_1000.npy' , 'rb') as f:\n",
    "#     raw_diff_1000= np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002118_2014-06-25_20-32-08_260595134347_rgbf000078-resize excluded\n",
      "002657_2014-06-13_15-29-54_094959634447_rgbf000150-resize excluded\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/pmahableshwarkar/Documents/Depth_Project/SUN_scene_cue_coding/GP_size_dict.json'\n",
    "# path = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/SUN_scene_cue_coding/GP_size_dict.json'\n",
    "\n",
    "gp_size_dict = json.load(open(path))\n",
    "\n",
    "# Find GP or VGP Outliers \n",
    "\n",
    "gpsize_list = [[key, gp_size_dict[key]] for key in gp_size_dict]\n",
    "gpsize_mean = np.mean(np.array([elem[1] for elem in gpsize_list]))\n",
    "gpsize_std = np.std(np.array([elem[1] for elem in gpsize_list]))\n",
    "\n",
    "gpsize_outlierRange = [gpsize_mean - (3*gpsize_std), gpsize_mean + (3*gpsize_std)]\n",
    "\n",
    "cleaned_gpsize_dict = {}\n",
    "for key in gp_size_dict:\n",
    "    if gpsize_outlierRange[0] < gp_size_dict[key] < gpsize_outlierRange[1]:\n",
    "        cleaned_gpsize_dict[key] = gp_size_dict[key]\n",
    "    else:\n",
    "        print(key, 'excluded')\n",
    "        \n",
    "ordered_gp_size = []\n",
    "gp_none = 0\n",
    "\n",
    "\n",
    "for folder in [elem.split('/')[1][:-2] for elem in stim_250]:\n",
    "    try:\n",
    "        ordered_gp_size.append(cleaned_gpsize_dict[folder])\n",
    "    except:\n",
    "        gp_none += 1\n",
    "        ordered_gp_size.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('/Users/pmahableshwarkar/Documents/Depth_Project/verbal_judgement_analysis/cue_analyses/cleaned_globalClutter.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "cleaned_globalClutter = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_globalClutter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 113, 113, 113, 113, 113)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalClutter = []\n",
    "\n",
    "completeGP = []\n",
    "\n",
    "trunc_residuals_125 = []\n",
    "trunc_residuals_250 = []\n",
    "trunc_residuals_1000 = []\n",
    "\n",
    "stimulus = []\n",
    "\n",
    "for stim in cleaned_globalClutter:\n",
    "    if stim in list(cleaned_gpsize_dict.keys()):\n",
    "        index_125 = [elem.split('/')[1][:-2] for elem in stim_125].index(stim)\n",
    "        index_250 = [elem.split('/')[1][:-2] for elem in stim_250].index(stim)\n",
    "        index_1000 = [elem.split('/')[1][:-2] for elem in stim_1000].index(stim)\n",
    "        \n",
    "        trunc_residuals_125.append(residuals_125[index_125])\n",
    "        trunc_residuals_250.append(residuals_250[index_250])\n",
    "        trunc_residuals_1000.append(residuals_1000[index_1000])\n",
    "        \n",
    "        globalClutter.append(cleaned_globalClutter[stim])\n",
    "        completeGP.append(cleaned_gpsize_dict[stim])\n",
    "        \n",
    "        stimulus.append(stim)\n",
    "        \n",
    "    \n",
    "len(trunc_residuals_125), len(trunc_residuals_250), len(trunc_residuals_1000), len(globalClutter), len(completeGP), len(stimulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([125] * len(trunc_residuals_125))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.concatenate((np.abs(np.array(trunc_residuals_125)), np.abs(np.array(trunc_residuals_250)), np.abs(np.array(trunc_residuals_1000))), axis=0),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>residuals</th>\n",
       "      <th>abs_residuals</th>\n",
       "      <th>duration</th>\n",
       "      <th>groundPlane</th>\n",
       "      <th>clutter</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.099975</td>\n",
       "      <td>0.099975</td>\n",
       "      <td>125</td>\n",
       "      <td>8.082259</td>\n",
       "      <td>3</td>\n",
       "      <td>000866_2014-06-09_20-45-42_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.080464</td>\n",
       "      <td>0.080464</td>\n",
       "      <td>125</td>\n",
       "      <td>10.664824</td>\n",
       "      <td>19</td>\n",
       "      <td>003070_2014-06-15_14-58-27_094959634447_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.301428</td>\n",
       "      <td>0.301428</td>\n",
       "      <td>125</td>\n",
       "      <td>9.164038</td>\n",
       "      <td>11</td>\n",
       "      <td>001092_2014-06-15_17-34-58_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.075657</td>\n",
       "      <td>0.075657</td>\n",
       "      <td>125</td>\n",
       "      <td>10.409246</td>\n",
       "      <td>14</td>\n",
       "      <td>002272_2014-06-28_18-53-56_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.164808</td>\n",
       "      <td>0.164808</td>\n",
       "      <td>125</td>\n",
       "      <td>31.362143</td>\n",
       "      <td>23</td>\n",
       "      <td>000642_2014-06-08_16-59-25_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.084265</td>\n",
       "      <td>0.084265</td>\n",
       "      <td>1000</td>\n",
       "      <td>17.196771</td>\n",
       "      <td>12</td>\n",
       "      <td>001094_2014-06-15_17-36-00_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.127303</td>\n",
       "      <td>0.127303</td>\n",
       "      <td>1000</td>\n",
       "      <td>20.030652</td>\n",
       "      <td>24</td>\n",
       "      <td>001808_2014-06-26_20-50-58_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.341774</td>\n",
       "      <td>0.341774</td>\n",
       "      <td>1000</td>\n",
       "      <td>11.913177</td>\n",
       "      <td>14</td>\n",
       "      <td>000758_2014-06-08_22-05-08_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.322175</td>\n",
       "      <td>0.322175</td>\n",
       "      <td>1000</td>\n",
       "      <td>5.576512</td>\n",
       "      <td>7</td>\n",
       "      <td>000878_2014-06-08_22-52-45_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>-0.224550</td>\n",
       "      <td>0.224550</td>\n",
       "      <td>1000</td>\n",
       "      <td>12.019051</td>\n",
       "      <td>12</td>\n",
       "      <td>001167_2014-06-17_15-38-07_260595134347_rgbf00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>339 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     residuals  abs_residuals  duration  groundPlane  clutter  \\\n",
       "0    -0.099975       0.099975       125     8.082259        3   \n",
       "1     0.080464       0.080464       125    10.664824       19   \n",
       "2    -0.301428       0.301428       125     9.164038       11   \n",
       "3    -0.075657       0.075657       125    10.409246       14   \n",
       "4    -0.164808       0.164808       125    31.362143       23   \n",
       "..         ...            ...       ...          ...      ...   \n",
       "334   0.084265       0.084265      1000    17.196771       12   \n",
       "335   0.127303       0.127303      1000    20.030652       24   \n",
       "336   0.341774       0.341774      1000    11.913177       14   \n",
       "337   0.322175       0.322175      1000     5.576512        7   \n",
       "338  -0.224550       0.224550      1000    12.019051       12   \n",
       "\n",
       "                                              stimulus  \n",
       "0    000866_2014-06-09_20-45-42_260595134347_rgbf00...  \n",
       "1    003070_2014-06-15_14-58-27_094959634447_rgbf00...  \n",
       "2    001092_2014-06-15_17-34-58_260595134347_rgbf00...  \n",
       "3    002272_2014-06-28_18-53-56_260595134347_rgbf00...  \n",
       "4    000642_2014-06-08_16-59-25_260595134347_rgbf00...  \n",
       "..                                                 ...  \n",
       "334  001094_2014-06-15_17-36-00_260595134347_rgbf00...  \n",
       "335  001808_2014-06-26_20-50-58_260595134347_rgbf00...  \n",
       "336  000758_2014-06-08_22-05-08_260595134347_rgbf00...  \n",
       "337  000878_2014-06-08_22-52-45_260595134347_rgbf00...  \n",
       "338  001167_2014-06-17_15-38-07_260595134347_rgbf00...  \n",
       "\n",
       "[339 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize data of lists.\n",
    "data = {'residuals': trunc_residuals_125+trunc_residuals_250+trunc_residuals_1000,\n",
    "        'abs_residuals': np.concatenate((np.abs(np.array(trunc_residuals_125)), np.abs(np.array(trunc_residuals_250)), np.abs(np.array(trunc_residuals_1000))), axis=0),\n",
    "        'duration': [125] * len(trunc_residuals_125) + [250] * len(trunc_residuals_250) + [1000] * len(trunc_residuals_1000),\n",
    "        'groundPlane': completeGP * 3,\n",
    "        'clutter': globalClutter * 3,\n",
    "        'stimulus': stimulus * 3}\n",
    "  \n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3230816540755546, 386.40077064565435, 7.3032069249418825, 8.380988898546363)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(np.array(df['residuals'])), np.std(np.array(df['duration'])), np.std(np.array(df['clutter'])), np.std(np.array(df['groundPlane']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('data_for_lme.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Participant Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/Users/pmahableshwarkar/Documents/Depth_Project/verbal_judgement_analysis/data/finalVEMatched/z_scored/residuals/participant_residuals.csv'\n",
    "\n",
    "p_data = pd.read_csv(p) \n",
    "p_stimuli = list(p_data.stimulus.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 43)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incl_stimuli = stimulus\n",
    "\n",
    "# images that we do not have ground plane and/or clutter data for \n",
    "excluded_stimuli = [x for x in p_stimuli if x not in incl_stimuli]\n",
    "\n",
    "len(excluded_stimuli), 156-113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all rows that have data for images that we do not have ground plane and/or clutter data for\n",
    "p_data = p_data[p_data.stimulus.isin(excluded_stimuli) == False]\n",
    "\n",
    "len(p_data.stimulus.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/create-new-column-based-on-other-columns-pandas-5586d87de73d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignGroundPlane(row):  \n",
    "    stim = row['stimulus']\n",
    "    groundPlane = cleaned_gpsize_dict[stim]\n",
    "    \n",
    "    return groundPlane\n",
    "\n",
    "def assignClutter(row):  \n",
    "    stim = row['stimulus']\n",
    "    clutter = cleaned_globalClutter[stim]\n",
    "    \n",
    "    return clutter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add groundPlane data to the df by referencing the GP value for each image\n",
    "p_data['groundPlane'] = p_data.apply(lambda row: assignGroundPlane(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clutter data to the df by referencing the GP value for each image\n",
    "p_data['clutter'] = p_data.apply(lambda row: assignClutter(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 20.030651550568866)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_globalClutter['001808_2014-06-26_20-50-58_260595134347_rgbf000029-resize'], cleaned_gpsize_dict['001808_2014-06-26_20-50-58_260595134347_rgbf000029-resize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data.to_csv('participantData_for_lme.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
