{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineCSVs(datafolder):\n",
    "    \"\"\"\n",
    "    Combine all participant data into one pandas df\n",
    "    OR \n",
    "    Create df for single participant file \n",
    "    \"\"\"\n",
    "    \n",
    "    exclude = []\n",
    "    \n",
    "    #checks if path is a file\n",
    "    isFile = os.path.isfile(datafolder)\n",
    "\n",
    "    #checks if path is a directory\n",
    "    \n",
    "    isDirectory = os.path.isdir(datafolder)\n",
    "    \n",
    "    if isDirectory == True:\n",
    "        data = []\n",
    "        for filename in os.listdir(datafolder):\n",
    "            if 'csv' in filename:\n",
    "                path = datafolder + \"/\" + filename\n",
    "                df = pd.read_csv(path, index_col=None, header=0)\n",
    "                \n",
    "                # do NOT include subject IDs that have been flagged \n",
    "                subjID = df.subjID.unique()[0]\n",
    "                if subjID not in exclude:\n",
    "                    data.append(df)\n",
    "\n",
    "                \n",
    "        input_frame = pd.concat(data, axis=0, ignore_index=True)\n",
    "        \n",
    "    if isFile == True:\n",
    "        if 'csv' in datafolder:\n",
    "            input_frame = pd.read_csv(datafolder, index_col=None, header=0)\n",
    "    \n",
    "    print('Number of participants before cleaning: ', len(input_frame.subjID.unique()))\n",
    "\n",
    " \n",
    "    return input_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '/Users/prachi/Documents/object_scene_scaling_data/pilot2'\n",
    "\n",
    "data_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/OSS_MTurk/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants before cleaning:  114\n"
     ]
    }
   ],
   "source": [
    "input_data = combineCSVs(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanbyPracticeTries(df, num_allowed_tries):\n",
    "    exclude = []\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    df2_list = []\n",
    "    prac_too_many_dist = []\n",
    "    for subj in all_subjIDs:\n",
    "        count = 0\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning \n",
    "        \n",
    "        subj_num_practice_tries = cleaned_subj_df.pracTries.unique()[0]\n",
    "        \n",
    "        if subj_num_practice_tries > num_allowed_tries:\n",
    "            prac_too_many_dist.append(subj_num_practice_tries)\n",
    "            remove.append(subj)\n",
    "        else:  \n",
    "            df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "            \n",
    "    print('Number of participants with more than ' + str(num_allowed_tries) + ' practice tries:', len(remove))\n",
    "    \n",
    "    exclude += remove \n",
    "    \n",
    "    return df2, prac_too_many_dist, exclude\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants with more than 3 practice tries: 4\n"
     ]
    }
   ],
   "source": [
    "pracTries_cleaned_data, prac_dist, exclude = cleanbyPracticeTries(input_data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pracTries_cleaned_data.subjID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy_Cleaning(df, accuracy_threshold, num_trials, exclude):\n",
    "    \"\"\"    \n",
    "    Remove participants with overall accuracy below the accuracy threshold (e.g. 0.7)\n",
    "    \n",
    "    Returns:\n",
    "        data of participants that passed the accuracy threshold \n",
    "        list of the number of correct trials each participant got\n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    df2_list = []\n",
    "    list_trials_correct = []\n",
    "    # number of participants where exclusion is because all responses were 'none'\n",
    "    c = 0\n",
    "    \n",
    "    for subj in all_subjIDs:\n",
    "        keypresses = []\n",
    "\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning \n",
    "        \n",
    "        subj_num_correct_trials = 0\n",
    "        \n",
    "        acc_column = np.array(list(subj_df['accuracy']))\n",
    "        sum_acc = np.sum(acc_column)\n",
    "        \n",
    "        \n",
    "        for idx, row in subj_df.iterrows():\n",
    "            trial_acc = row['accuracy']\n",
    "            if trial_acc == 1:\n",
    "                subj_num_correct_trials += 1\n",
    "            else:\n",
    "                keypresses.append(row['keyPress'])\n",
    "        \n",
    "        subj_acc = sum_acc/num_trials\n",
    "#         print(acc_column)\n",
    "#         print(subj, sum_acc, subj_acc)\n",
    "                \n",
    "        # minimum number of trials correct the participant must have to be included\n",
    "        if subj_acc < accuracy_threshold:\n",
    "#             print(subj_acc, subj)\n",
    "            remove.append(subj)\n",
    "#             print(len([x for x in keypresses if x == 'none']))\n",
    "            if len([x for x in keypresses if x == 'none']) >= 4:\n",
    "                c += 1\n",
    "            \n",
    "#         else:\n",
    "#             print(subj_acc)\n",
    "        \n",
    "        list_trials_correct.append(subj_num_correct_trials)\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "    \n",
    "    print(\"Number of Participants with accuracy below 70%: \", len(remove))\n",
    "    \n",
    "    for index, row in df2.iterrows():\n",
    "        if row['subjID'] in remove:\n",
    "            df2.drop(index, inplace=True)\n",
    "                \n",
    "    print('Number of participants that did not respond for 4 or more trials:', c)\n",
    "    \n",
    "    print('Number of participants left: ', len(df2.subjID.unique()))\n",
    "    \n",
    "    exclude += remove\n",
    "                \n",
    "    return df2, list_trials_correct, keypresses, exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Participants with accuracy below 70%:  4\n",
      "Number of participants that did not respond for 4 or more trials: 2\n",
      "Number of participants left:  106\n"
     ]
    }
   ],
   "source": [
    "Accuracy_cleaned_data, correct_trials_distribution, keypresses, exclude = Accuracy_Cleaning(pracTries_cleaned_data, 0.7, num_trials, exclude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RT_Cleaning(df, outlier_range, num_trials, exclude):\n",
    "    \"\"\"\n",
    "    Remove trials where trial RT is outside of the defined outlier range \n",
    "    \n",
    "    Returns:\n",
    "        dataframe with outlier RT trials removed\n",
    "        list of all RTs \n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    print(len(all_subjIDs))\n",
    "    \n",
    "    remove = []\n",
    "    df2_list = []\n",
    "    total_RT_outliers = 0\n",
    "    total = 0\n",
    "    list_trialRT = []\n",
    "    for subj in all_subjIDs:\n",
    "        count = 0\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning \n",
    "\n",
    "        for idx, row in subj_df.iterrows():\n",
    "            total += 1\n",
    "            RT = row[\"RT\"]\n",
    "            list_trialRT.append(RT)\n",
    "            if RT < outlier_range[0]: # outlier\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "                total_RT_outliers += 1\n",
    "            if RT > outlier_range[1]:\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "                total_RT_outliers += 1\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "    print(len(df2.subjID.unique()))\n",
    "    \n",
    "    exclude += remove\n",
    "                \n",
    "    return df2, list_trialRT, exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "RT_cleaned_data, trialRTs_distribution, exclude = RT_Cleaning(Accuracy_cleaned_data, [250, 5000], num_trials, exclude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalTrialCountCheck(df, num_trials, min_trials, exclude):\n",
    "    \"\"\"\n",
    "    If more then 10% of a participants data is missing, remove the participant\n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        count_trials = len(subj_df.index)\n",
    "        if count_trials < min_trials:\n",
    "            remove.append(subj)\n",
    "            \n",
    "#         threshold_trials_remaining = num_trials - math.floor(num_trials * 0.1)\n",
    "\n",
    "#         if count_trials <= threshold_trials_remaining:\n",
    "#             remove.append(subj)\n",
    "            \n",
    "    print(\"Number of Participants with >= 10% trials removed: \", len(remove))\n",
    "            \n",
    "    for subj in remove:\n",
    "        df.drop(df[df['subjID'] == subj].index, inplace = True) \n",
    "                    \n",
    "    print(\"Number of participants left: \",len(df.subjID.unique()))\n",
    "    \n",
    "    exclude += remove \n",
    "    return df, exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Participants with >= 10% trials removed:  10\n",
      "Number of participants left:  96\n"
     ]
    }
   ],
   "source": [
    "finalTrialCount_data, exclude = finalTrialCountCheck(RT_cleaned_data, num_trials, 22, exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_final_data = finalTrialCount_data.copy(deep=True)\n",
    "len(raw_final_data.subjID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jsons/s7.json': 4,\n",
       " 'jsons/s2.json': 4,\n",
       " 'jsons/s13.json': 4,\n",
       " 'jsons/s1.json': 4,\n",
       " 'jsons/s17.json': 4,\n",
       " 'jsons/s4.json': 4,\n",
       " 'jsons/s12.json': 4,\n",
       " 'jsons/s3.json': 4,\n",
       " 'jsons/s0.json': 4,\n",
       " 'jsons/s19.json': 4,\n",
       " 'jsons/s23.json': 4,\n",
       " 'jsons/s20.json': 4,\n",
       " 'jsons/s21.json': 4,\n",
       " 'jsons/s14.json': 4,\n",
       " 'jsons/s6.json': 4,\n",
       " 'jsons/s10.json': 4,\n",
       " 'jsons/s5.json': 4,\n",
       " 'jsons/s8.json': 4,\n",
       " 'jsons/s18.json': 4,\n",
       " 'jsons/s9.json': 4,\n",
       " 'jsons/s22.json': 4,\n",
       " 'jsons/s11.json': 4,\n",
       " 'jsons/s16.json': 4,\n",
       " 'jsons/s15.json': 4}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = raw_final_data.sequenceName.unique()\n",
    "seq_track = {}\n",
    "for seq in seqs:\n",
    "    sq_df = raw_final_data.loc[raw_final_data['sequenceName']==seq]\n",
    "    seq_track[seq] = len(sq_df.subjID.unique())\n",
    "\n",
    "seq_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_to_replace = ['jsons/s10.json', 'jsons/s11.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jsons/s0.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jsons/s1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jsons/s2.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jsons/s3.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jsons/s4.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jsons/s5.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jsons/s6.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jsons/s7.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jsons/s8.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jsons/s9.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jsons/s10.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jsons/s11.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jsons/s12.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jsons/s13.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jsons/s14.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>jsons/s15.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>jsons/s16.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>jsons/s17.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>jsons/s18.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>jsons/s19.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>jsons/s20.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>jsons/s21.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>jsons/s22.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>jsons/s23.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Path\n",
       "0    jsons/s0.json\n",
       "1    jsons/s1.json\n",
       "2    jsons/s2.json\n",
       "3    jsons/s3.json\n",
       "4    jsons/s4.json\n",
       "5    jsons/s5.json\n",
       "6    jsons/s6.json\n",
       "7    jsons/s7.json\n",
       "8    jsons/s8.json\n",
       "9    jsons/s9.json\n",
       "10  jsons/s10.json\n",
       "11  jsons/s11.json\n",
       "12  jsons/s12.json\n",
       "13  jsons/s13.json\n",
       "14  jsons/s14.json\n",
       "15  jsons/s15.json\n",
       "16  jsons/s16.json\n",
       "17  jsons/s17.json\n",
       "18  jsons/s18.json\n",
       "19  jsons/s19.json\n",
       "20  jsons/s20.json\n",
       "21  jsons/s21.json\n",
       "22  jsons/s22.json\n",
       "23  jsons/s23.json"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterbalancing_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/OSS_MTurk/counterbalancing.csv'\n",
    "counterbalancing_df = pd.read_csv(counterbalancing_path)\n",
    "counterbalancing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jsons/s10.json'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_to_replace[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_fragments = []\n",
    "for sequence in sequences_to_replace:\n",
    "    url_fragments.append(counterbalancing_df.index[counterbalancing_df['Path']==sequence][0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number for the NEXT batch \n",
    "batch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_variables_csv = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/OSS_MTurk/batch_variables/'\n",
    "\n",
    "base_url = 'http://54.235.29.9/FacialAge/OSS_MTurk/OSS_HTML_e5v2.html#'\n",
    "\n",
    "variables = {'experiment_url': []}\n",
    "\n",
    "for fragment in url_fragments:\n",
    "     variables['experiment_url'].append(base_url + str(fragment))\n",
    "\n",
    "variables_df = pd.DataFrame(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_df.to_csv(dest_variables_csv + 'e5v2' + '_b' + str(batch) + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
